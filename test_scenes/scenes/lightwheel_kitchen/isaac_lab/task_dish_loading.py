"""
Task Implementation - kitchen_dish_loading
Generated by BlueprintRecipe

This file implements the task logic for Dish Loading.
"""

from __future__ import annotations

import torch
from typing import TYPE_CHECKING

from omni.isaac.lab.envs import ManagerBasedEnv
from . import reward_functions

if TYPE_CHECKING:
    from .env_cfg import KitchenDishLoadingEnvCfg


class KitchenDishLoadingTask:
    """
    Task implementation for kitchen_dish_loading.

    Policy: Dish Loading
    Description: Loading dishes into dishwashers from various locations
    """

    def __init__(self, env: ManagerBasedEnv, cfg: KitchenDishLoadingEnvCfg):
        self.env = env
        self.cfg = cfg
        self.device = env.device
        self.num_envs = env.num_envs
        # Get scene entity map from config (set in config's __post_init__)
        self.scene_entity_map = getattr(cfg, "scene_entity_map", {'robot': '/World/Robot', 'scene_root': '/World/Kitchen', 'Refrigerator001': '/World/Kitchen/Objects/Refrigerator001', 'Dishwasher054': '/World/Kitchen/Objects/Dishwasher054', 'Microwave017': '/World/Kitchen/Objects/Microwave017', 'Sink054': '/World/Kitchen/Objects/Sink054', 'Stovetop012': '/World/Kitchen/Objects/Stovetop012', 'RangeHood015': '/World/Kitchen/Objects/RangeHood015', 'Kitchen_Cabinet001': '/World/Kitchen/Objects/Kitchen_Cabinet001', 'Kitchen_Cabinet002': '/World/Kitchen/Objects/Kitchen_Cabinet002', 'Table049': '/World/Kitchen/Objects/Table049', 'WallStackOven004': '/World/Kitchen/Objects/WallStackOven004', 'Toaster003': '/World/Kitchen/Objects/Toaster003', 'CoffeeMachine006': '/World/Kitchen/Objects/CoffeeMachine006', 'Pot057': '/World/Kitchen/Objects/Pot057'})
        self.target_name = "Refrigerator001"
        self.ee_frame = "panda_hand"

        # Initialize reward function state variables (CRITICAL for sim2real transfer rewards)
        reward_functions.initialize_reward_state(env)

        # Task state
        self._setup_task_state()

    def _setup_task_state(self):
        """Initialize task-specific state tensors."""
        self.task_success = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)
        self.object_dropped = torch.zeros(self.num_envs, dtype=torch.bool, device=self.device)

    def reset(self, env_ids: torch.Tensor):
        """Reset task state for specified environments."""
        self.task_success[env_ids] = False
        self.object_dropped[env_ids] = False

    def compute_observations(self) -> dict[str, torch.Tensor]:
        """Compute task-specific observations and merge with manager outputs."""

        observations = self.env.observation_manager.compute()
        target = self._get_scene_entity(self.target_name)
        if target is not None:
            target_pose = torch.cat([target.data.root_pos_w, target.data.root_quat_w], dim=-1)
            observations["target_pose"] = target_pose

        return observations

    def compute_rewards(self) -> dict[str, torch.Tensor]:
        """Compute reward components and integrate with RewardManager."""

        reward_terms = self.env.reward_manager.compute()
        target = self._get_scene_entity(self.target_name)
        robot = self.env.scene.get("robot")

        if target is not None and robot is not None:
            ee_pos = self._get_ee_pos(robot)
            dist_to_target = torch.norm(target.data.root_pos_w - ee_pos, dim=-1)
            dense_reach = 1.0 - torch.tanh(4.0 * dist_to_target)
            close_enough = dist_to_target < 0.05

            reward_terms.setdefault("dense_reach", dense_reach)
            reward_terms.setdefault("proximity_success", close_enough.float() * 5.0)
            self.task_success = self.task_success | close_enough

        total = torch.zeros(self.num_envs, device=self.device)
        for value in reward_terms.values():
            total = total + value
        reward_terms["total"] = total
        return reward_terms

    def compute_terminations(self) -> dict[str, torch.Tensor]:
        """Compute termination conditions including manager-driven terms."""

        terminations = self.env.termination_manager.compute()
        terminations.setdefault("task_success", self.task_success)

        target = self._get_scene_entity(self.target_name)
        if target is not None:
            dropped = target.data.root_pos_w[:, 2] < -0.05
            self.object_dropped = self.object_dropped | dropped
            terminations.setdefault("object_dropped", dropped)

        return terminations

    def _get_ee_pos(self, robot=None) -> torch.Tensor:
        """Get end-effector position."""
        robot = robot or self.env.scene.get("robot")
        return robot.data.body_pos_w[:, robot.find_bodies(self.ee_frame)[0]]

    def _get_scene_entity(self, name: str):
        """Resolve a scene entity using the generated map for prim paths."""

        entity = self.env.scene.get(name)
        if entity is not None:
            return entity

        prim_path = self.scene_entity_map.get(name)
        if prim_path:
            return self.env.scene.get(prim_path)
        return None
