# Training Configuration - bedroom_manipulation
# Generated by BlueprintRecipe

# Task settings
task_name: "bedroom_manipulation"
experiment_name: "bedroom_manipulation_training"

# Device settings
device: "cuda:0"  # Use GPU 0 by default, can be overridden

# Environment settings
env:
  num_envs: 1024
  episode_length: 500

# PPO Algorithm settings
algo:
  name: "PPO"
  policy:
    class_name: "ActorCritic"
    init_noise_std: 1.0
    actor_hidden_dims: [256, 256, 128]
    critic_hidden_dims: [256, 256, 128]
    activation: "elu"

  # PPO specific
  clip_param: 0.2
  entropy_coef: 0.01
  value_loss_coef: 1.0
  max_grad_norm: 1.0

  # KL divergence early stopping (prevents policy from changing too quickly)
  desired_kl: 0.01
  max_kl: 0.02

  # Learning rate
  learning_rate: 3.0e-4
  lr_schedule: "adaptive"

  # Batch settings
  rollout_steps: 500  # Number of steps to collect before training
  batch_size: 512000  # Total samples per iteration
  num_learning_epochs: 5
  num_mini_batches: 4

  # Discount
  gamma: 0.99
  lam: 0.95

# Training settings
runner:
  max_iterations: 1500
  save_interval: 100
  log_interval: 10

  # Checkpointing
  checkpoint_path: null
  resume: false

# Logging
logging:
  wandb:
    enabled: false
    project: "blueprint_recipe"
    entity: null
  tensorboard:
    enabled: true

# Reward weights
rewards:
  action_jerk_penalty: 0.01
  action_magnitude_penalty: 0.001

# Domain randomization
randomization:
  enabled: true
  on_reset:
    []
  on_step:
    []
