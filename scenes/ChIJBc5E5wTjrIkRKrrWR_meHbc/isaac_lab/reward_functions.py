"""
Reward Functions Module
Generated by BlueprintRecipe

This module contains reward function implementations for the task.
"""

from __future__ import annotations

import torch
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedEnv


def initialize_reward_state(env: ManagerBasedEnv) -> None:
    """
    Initialize state variables required by reward functions.

    This function should be called during environment initialization
    to prevent AttributeError on first timestep.

    Args:
        env: The environment instance to initialize
    """
    # Initialize action history for jerk/rate penalties
    if hasattr(env, "action_manager") and env.action_manager.action is not None:
        env._prev_actions = env.action_manager.action.clone()
        env._prev_actions_rate = env.action_manager.action.clone()
    else:
        # Fallback if action_manager not yet initialized
        action_dim = 7  # Default for most manipulation robots
        env._prev_actions = torch.zeros(env.num_envs, action_dim, device=env.device)
        env._prev_actions_rate = torch.zeros(env.num_envs, action_dim, device=env.device)

    # Initialize end-effector velocity history for smoothness penalties
    robot = env.scene.get("robot")
    if robot is not None and hasattr(robot, "data"):
        try:
            ee_body_idx = robot.find_bodies("panda_hand")[0] if hasattr(robot, "find_bodies") else 0
            env._prev_ee_vel = robot.data.body_vel_w[:, ee_body_idx, :3].clone()
        except (IndexError, AttributeError):
            # Fallback if end-effector not found
            env._prev_ee_vel = torch.zeros(env.num_envs, 3, device=env.device)
    else:
        env._prev_ee_vel = torch.zeros(env.num_envs, 3, device=env.device)



def reward_action_jerk_penalty(
    env: ManagerBasedEnv,
    penalty_scale: float = 0.01,
) -> torch.Tensor:
    """
    Penalty for high-frequency action changes (jerk).

    This is CRITICAL for sim2real transfer because:
    1. Real actuators have rate limits and cannot change instantly
    2. Policies that "vibrate" exploit numerical solver artifacts
    3. Smooth actions transfer better to real hardware

    The penalty is computed as the squared difference between
    consecutive actions, encouraging smooth action trajectories.

    Args:
        env: The environment instance
        penalty_scale: Scale factor for the penalty (default: 0.01)

    Returns:
        Negative reward proportional to action jerk
    """
    # Get current and previous actions
    current_actions = env.action_manager.action
    prev_actions = getattr(env, "_prev_actions", current_actions)

    # Store current actions for next step
    env._prev_actions = current_actions.clone()

    # Compute action difference (jerk proxy)
    action_diff = current_actions - prev_actions

    # L2 norm of action change
    jerk = torch.sum(action_diff ** 2, dim=-1)

    return -penalty_scale * jerk


def reward_action_magnitude_penalty(
    env: ManagerBasedEnv,
    penalty_scale: float = 0.001,
) -> torch.Tensor:
    """
    Penalty for large action magnitudes.

    Encourages energy-efficient actions and prevents policies
    from learning to saturate actuators constantly.

    Args:
        env: The environment instance
        penalty_scale: Scale factor for the penalty

    Returns:
        Negative reward proportional to action magnitude
    """
    actions = env.action_manager.action
    magnitude = torch.sum(actions ** 2, dim=-1)
    return -penalty_scale * magnitude


def compute_combined_reward(
    env: ManagerBasedEnv,
) -> torch.Tensor:
    """Compute weighted sum of all reward components."""
    weights = {
        "action_jerk_penalty": 0.01,
        "action_magnitude_penalty": 0.001,
    }

    total_reward = torch.zeros(env.num_envs, device=env.device)

    for component, weight in weights.items():
        func = globals().get(f"reward_{component}")
        if func is not None:
            total_reward += weight * func(env)

    return total_reward
