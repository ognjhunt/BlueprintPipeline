# =============================================================================
# Training Pipeline
# =============================================================================
#
# Trains robot policies on imported episodes from Genie Sim 3.0.
#
# Trigger Methods:
# 1. Eventarc event from genie-sim-import-pipeline:
#    Event type: "blueprintpipeline.episodes.imported"
#
# 2. Manual trigger:
#    gcloud eventarc triggers create training-trigger \
#      --destination-workflow=training-pipeline \
#      --event-filters="type=blueprintpipeline.episodes.imported"
#
# Pipeline Flow:
#   Episodes Imported → Validate Dataset → Launch Training Job → Monitor Training
#

main:
  params: [event]
  steps:
    - init:
        assign:
          - project_id: ${sys.get_env("GOOGLE_CLOUD_PROJECT")}
          - region: ${default(sys.get_env("WORKFLOW_REGION"), "us-central1")}
          - bucket: ${default(sys.get_env("PRIMARY_BUCKET"), sys.get_env("GCS_BUCKET", "blueprintpipeline-data"))}
          - trainingJobName: "training-job"
          # Timeout defaults align with policy_configs/adaptive_timeouts.yaml unless overridden.
          - adaptiveTimeoutConfigPath: "policy_configs/adaptive_timeouts.yaml"
          - adaptiveTimeoutDefaultSeconds: 1800
          - trainingTimeoutSeconds: 21600

    - extract_event:
        switch:
          # Direct event payload
          - condition: ${"job_id" in event}
            assign:
              - job_id: ${event.job_id}
              - scene_id: ${default(event.scene_id, "unknown")}
              - output_path: ${event.output_path}
              - episode_count: ${default(event.episode_count, 0)}
              - timeout_override_seconds: ${default(event.timeout_override_seconds, null)}
              - trainingTimeoutSeconds: ${default(event.timeout_override_seconds, trainingTimeoutSeconds)}

          # Wrapped Eventarc event
          - condition: ${"data" in event}
            assign:
              - job_id: ${event.data.job_id}
              - scene_id: ${default(event.data.scene_id, "unknown")}
              - output_path: ${event.data.output_path}
              - episode_count: ${default(event.data.episode_count, 0)}
              - timeout_override_seconds: ${default(event.data.timeout_override_seconds, null)}
              - trainingTimeoutSeconds: ${default(event.data.timeout_override_seconds, trainingTimeoutSeconds)}

          # Default/fallback
          - condition: true
            steps:
              - log_malformed_event:
                  call: sys.log
                  args:
                    data:
                      bp_metric: "malformed_event"
                      workflow: "training-pipeline"
                      required_fields:
                        - job_id
                        - output_path
                      error:
                        code: "invalid_event_payload"
                        message: "Missing required event data. Event must contain job_id and output_path."
                      remediation: "Ensure the Eventarc payload includes job_id and output_path in event or event.data."
                      raw_event: ${event}
                    severity: ERROR
              - return_invalid_event:
                  return:
                    status: "error"
                    error:
                      code: "invalid_event_payload"
                      message: "Missing required event data. Event must contain job_id and output_path."
                      required_fields:
                        - job_id
                        - output_path
                      remediation: "Ensure the Eventarc payload includes job_id and output_path in event or event.data."
                      raw_event: ${event}

    - validate_inputs:
        switch:
          - condition: ${job_id == null or job_id == ""}
            steps:
              - log_missing_job_id:
                  call: sys.log
                  args:
                    data:
                      bp_metric: "malformed_event"
                      workflow: "training-pipeline"
                      required_fields:
                        - job_id
                      error:
                        code: "missing_job_id"
                        message: "job_id is required."
                      remediation: "Provide job_id in the event payload (event.job_id or event.data.job_id)."
                      raw_event: ${event}
                    severity: ERROR
              - return_missing_job_id:
                  return:
                    status: "error"
                    error:
                      code: "missing_job_id"
                      message: "job_id is required."
                      required_fields:
                        - job_id
                      remediation: "Provide job_id in the event payload (event.job_id or event.data.job_id)."
                      raw_event: ${event}
          - condition: ${output_path == null or output_path == ""}
            steps:
              - log_missing_output_path:
                  call: sys.log
                  args:
                    data:
                      bp_metric: "malformed_event"
                      workflow: "training-pipeline"
                      required_fields:
                        - output_path
                      error:
                        code: "missing_output_path"
                        message: "output_path is required."
                      remediation: "Provide output_path in the event payload (event.output_path or event.data.output_path)."
                      raw_event: ${event}
                    severity: ERROR
              - return_missing_output_path:
                  return:
                    status: "error"
                    error:
                      code: "missing_output_path"
                      message: "output_path is required."
                      required_fields:
                        - output_path
                      remediation: "Provide output_path in the event payload (event.output_path or event.data.output_path)."
                      raw_event: ${event}
        next: set_training_markers

    - set_training_markers:
        assign:
          - trainingFailedMarker: ${output_path + "/.failed"}
        next: init_quality_gate_preflight

    # Preflight quality gate check (downstream guardrail)
    - init_quality_gate_preflight:
        assign:
          - qualityGateReportObject: ${"scenes/" + scene_id + "/episode-generation-job/quality_gate_report.json"}
          - qualityGateReportPath: ${"gs://" + bucket + "/" + qualityGateReportObject}
          - qualityGateFailedMarker: ${"scenes/" + scene_id + "/training/.failed"}
          - qualityCertPrefix: ${"scenes/" + scene_id + "/episodes/"}
          - qualityCertObject: null
          - qualityCertPageToken: null
        next: maybe_check_quality_gates

    - maybe_check_quality_gates:
        switch:
          - condition: ${scene_id == null or scene_id == "" or scene_id == "unknown"}
            next: log_quality_gate_skip
        next: read_quality_gate_report

    - log_quality_gate_skip:
        call: sys.log
        args:
          text: "Skipping quality gate preflight; scene_id is missing."
          severity: "WARNING"
        next: log_start

    - read_quality_gate_report:
        try:
          call: googleapis.storage.v1.objects.get
          args:
            bucket: ${bucket}
            object: ${qualityGateReportObject}
            alt: "media"
          result: qualityGateReportRaw
        except:
          as: e
          steps:
            - handle_quality_gate_report_missing:
                switch:
                  - condition: ${e.code == 404}
                    next: list_quality_certificates
                next: raise_quality_gate_report_error

    - raise_quality_gate_report_error:
        raise:
          message: ${"Failed to read quality gate report at " + qualityGateReportPath}

    - parse_quality_gate_report:
        assign:
          - qualityGateReport: ${json.decode(qualityGateReportRaw)}
        next: evaluate_quality_gate_report

    - evaluate_quality_gate_report:
        switch:
          - condition: ${"summary" in qualityGateReport and qualityGateReport.summary.can_proceed == true and qualityGateReport.summary.blocking_failures == 0}
            next: log_quality_gate_passed
        next: quality_gate_failed

    - list_quality_certificates:
        call: googleapis.storage.v1.objects.list
        args:
          bucket: ${bucket}
          prefix: ${qualityCertPrefix}
          maxResults: 1000
          pageToken: ${qualityCertPageToken}
        result: qualityCertList
        next: scan_quality_certificates

    - scan_quality_certificates:
        for:
          value: certItem
          in: ${default(qualityCertList.items, [])}
          steps:
            - check_quality_cert_match:
                switch:
                  - condition: ${qualityCertObject == null and text.match_regex(certItem.name, "quality_certificate\\.json$")}
                    assign:
                      - qualityCertObject: ${certItem.name}
        next: check_quality_cert_scan_results

    - check_quality_cert_scan_results:
        switch:
          - condition: ${qualityCertObject != null}
            next: read_quality_certificate
          - condition: ${qualityCertList.nextPageToken != null}
            assign:
              - qualityCertPageToken: ${qualityCertList.nextPageToken}
            next: list_quality_certificates
        next: quality_gate_missing_artifacts

    - read_quality_certificate:
        call: googleapis.storage.v1.objects.get
        args:
          bucket: ${bucket}
          object: ${qualityCertObject}
          alt: "media"
        result: qualityCertRaw
        next: parse_quality_certificate

    - parse_quality_certificate:
        assign:
          - qualityCertificate: ${json.decode(qualityCertRaw)}
        next: evaluate_quality_certificate

    - evaluate_quality_certificate:
        switch:
          - condition: ${qualityCertificate.validation_passed == true and qualityCertificate.recommended_use != "testing"}
            next: log_quality_gate_passed_with_certificate
        next: quality_gate_failed

    - log_quality_gate_passed_with_certificate:
        call: sys.log
        args:
          text: ${"Quality gate report missing; proceeding based on certificate " + qualityCertObject}
          severity: "WARNING"
        next: log_start

    - quality_gate_missing_artifacts:
        call: sys.log
        args:
          text: ${"Missing quality gate report and quality certificates for scene " + scene_id}
          severity: "ERROR"
        next: quality_gate_failed

    - log_quality_gate_passed:
        call: sys.log
        args:
          text: ${"Quality gate passed for scene " + scene_id}
          severity: "INFO"
        next: log_start

    - quality_gate_failed:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${bucket}
          name: ${qualityGateFailedMarker}
          uploadType: "media"
          body: ${json.encode({
            "scene_id": scene_id,
            "status": "failed",
            "timestamp": time.format(sys.now()),
            "error": {
              "code": "quality_gate_failed",
              "message": "Downstream quality gate preflight blocked training",
              "report_path": qualityGateReportPath
            }
          })}
        result: qualityGateFailedMarkerResult
        next: raise_quality_gate_failed

    - raise_quality_gate_failed:
        raise:
          message: ${"Quality gate failed; report: " + qualityGateReportPath}

    - log_start:
        call: sys.log
        args:
          text: ${"Starting training pipeline for job " + job_id + " with " + string(episode_count) + " episodes from " + output_path}
          severity: INFO

    # Validate dataset exists and has minimum episodes
    - validate_dataset:
        switch:
          - condition: ${episode_count < 1}
            steps:
              - log_insufficient_episodes:
                  call: sys.log
                  args:
                    text: ${"Insufficient episodes (" + string(episode_count) + ") for training. Minimum 1 required."}
                    severity: WARNING
              - return_skipped:
                  return:
                    status: "skipped"
                    job_id: ${job_id}
                    reason: "Insufficient episodes for training"

    - init_training_metrics:
        assign:
          - trainingStartTime: '${time.format(sys.now())}'
        next: emit_training_metrics_start

    - emit_training_metrics_start:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "start"
            workflow: "training-pipeline"
            job: ${trainingJobName}
            scene_id: ${scene_id}
            timeout_seconds: ${trainingTimeoutSeconds}
            duration_seconds: 0
            timeout_usage_ratio: 0
            timed_out: false
            status: "STARTED"
            start_time: ${trainingStartTime}
          severity: "INFO"

    # Launch training job
    - run_training_job:
        try:
          call: googleapis.run.v2.projects.locations.jobs.run
          args:
            name: ${"projects/" + project_id + "/locations/" + region + "/jobs/" + trainingJobName}
            body:
              overrides:
                containerOverrides:
                  - name: "training"
                    env:
                      - name: BUCKET
                        value: ${bucket}
                      - name: SCENE_ID
                        value: ${scene_id}
                      - name: OUTPUT_PATH
                        value: ${output_path}
                      - name: EPISODE_COUNT
                        value: ${string(episode_count)}
                      - name: TRAINING_MODE
                        value: "lerobot"
                      - name: MODEL_ARCHITECTURE
                        value: "act"  # Default to ACT (Action Chunking Transformer)
                      - name: TRAINING_EPOCHS
                        value: "100"
                      - name: BATCH_SIZE
                        value: "32"
                      - name: LEARNING_RATE
                        value: "0.0001"
                      - name: CHECKPOINT_INTERVAL
                        value: "10"
                timeout: '${string(trainingTimeoutSeconds) + "s"}'
          result: training_job_execution
        retry:
          predicate: ${http.default_retry_predicate}
          max_retries: 5  # default per policy_configs/retry_policy.yaml
          backoff:
            initial_delay: 1
            max_delay: 60
            multiplier: 2
        except:
          as: e
          steps:
            - log_training_retry_exhausted:
                call: sys.log
                args:
                  data:
                    bp_metric: "job_retry_exhausted"
                    workflow: "training-pipeline"
                    job: ${trainingJobName}
                    scene_id: ${scene_id}
                    retry_max: 5
                    error: ${e.message}
                  severity: "ERROR"
            - list_training_outputs_for_cleanup:
                call: googleapis.storage.v1.objects.list
                args:
                  bucket: ${bucket}
                  prefix: ${output_path + "/"}
                  maxResults: 1000
                result: trainingCleanupList
            - delete_training_outputs_for_cleanup:
                for:
                  value: trainingCleanupItem
                  in: ${default(trainingCleanupList.items, [])}
                  steps:
                    - delete_training_output:
                        call: googleapis.storage.v1.objects.delete
                        args:
                          bucket: ${bucket}
                          object: ${trainingCleanupItem.name}
            - capture_training_failure_metrics:
                assign:
                  - trainingEndTime: '${time.format(sys.now())}'
                  - trainingDurationSeconds: '${time.parse(trainingEndTime) - time.parse(trainingStartTime)}'
                  - trainingTimeoutUsageRatio: '${trainingDurationSeconds / trainingTimeoutSeconds}'
                  - trainingTimedOut: '${trainingDurationSeconds >= trainingTimeoutSeconds}'
            - emit_training_metrics_failed_on_start:
                call: sys.log
                args:
                  data:
                    bp_metric: "job_invocation"
                    event: "complete"
                    workflow: "training-pipeline"
                    job: ${trainingJobName}
                    scene_id: ${scene_id}
                    timeout_seconds: ${trainingTimeoutSeconds}
                    duration_seconds: ${trainingDurationSeconds}
                    timeout_usage_ratio: ${trainingTimeoutUsageRatio}
                    timed_out: ${trainingTimedOut}
                    status: "FAILED"
                    start_time: ${trainingStartTime}
                    end_time: ${trainingEndTime}
                  severity: "ERROR"
            - write_training_failure_marker:
                call: googleapis.storage.v1.objects.insert
                args:
                  bucket: ${bucket}
                  name: ${trainingFailedMarker}
                  uploadType: "media"
                  body: ${json.encode({
                    "job_id": job_id,
                    "scene_id": scene_id,
                    "job_name": trainingJobName,
                    "status": "failed",
                    "timestamp": time.format(sys.now()),
                    "error": {
                      "code": "training_start_failed",
                      "message": e.message,
                      "type": "workflow_failure",
                      "stack_trace": null
                    },
                    "context": {
                      "workflow_execution_id": sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),
                      "attempt_number": 1,
                      "config_context": {
                        "output_path": output_path
                      }
                    }
                  })}
            - raise_training_job_error:
                raise: ${e}

    - set_training_execution_name:
        assign:
          - trainingExecutionName: '${if(training_job_execution.metadata != null and training_job_execution.metadata.name != null, training_job_execution.metadata.name, training_job_execution.name)}'
        next: wait_for_training

    - wait_for_training:
        call: googleapis.run.v2.projects.locations.jobs.executions.get
        args:
          name: ${trainingExecutionName}
        result: training_status
        next: check_training_status

    - check_training_status:
        assign:
          - trainingState: '${if(training_status.state != null, training_status.state, if(training_status.status != null, training_status.status.state, null))}'
          - trainingFailedCount: '${if(training_status.failedCount != null, training_status.failedCount, if(training_status.status != null, training_status.status.failedCount, null))}'
          - trainingSucceededCount: '${if(training_status.succeededCount != null, training_status.succeededCount, if(training_status.status != null, training_status.status.succeededCount, null))}'
        next: training_status_switch

    - training_status_switch:
        switch:
          - condition: '${trainingState == "FAILED" or (trainingFailedCount != null and trainingFailedCount > 0)}'
            next: check_training_failure
          - condition: '${trainingState == "SUCCEEDED" or (trainingSucceededCount != null and trainingSucceededCount > 0)}'
            next: log_training_success
        next: wait_training_poll

    - wait_training_poll:
        call: sys.sleep
        args:
          seconds: 30  # Poll every 30 seconds (training takes longer)
        next: wait_for_training

    - check_training_failure:
        assign:
          - trainingEndTime: '${time.format(sys.now())}'
          - trainingDurationSeconds: '${time.parse(trainingEndTime) - time.parse(trainingStartTime)}'
          - trainingTimeoutUsageRatio: '${trainingDurationSeconds / trainingTimeoutSeconds}'
          - trainingTimedOut: '${trainingDurationSeconds >= trainingTimeoutSeconds}'
        next: emit_training_metrics_failed

    - emit_training_metrics_failed:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "complete"
            workflow: "training-pipeline"
            job: ${trainingJobName}
            scene_id: ${scene_id}
            timeout_seconds: ${trainingTimeoutSeconds}
            duration_seconds: ${trainingDurationSeconds}
            timeout_usage_ratio: ${trainingTimeoutUsageRatio}
            timed_out: ${trainingTimedOut}
            status: "FAILED"
            start_time: ${trainingStartTime}
            end_time: ${trainingEndTime}
          severity: "ERROR"
        next: log_training_failed_message

    - log_training_failed_message:
        call: sys.log
        args:
          text: ${"Training job failed for job " + job_id}
          severity: ERROR
        next: write_training_failure_marker_on_failure

    - write_training_failure_marker_on_failure:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${bucket}
          name: ${trainingFailedMarker}
          uploadType: "media"
          body: ${json.encode({
            "job_id": job_id,
            "scene_id": scene_id,
            "job_name": trainingJobName,
            "status": "failed",
            "timestamp": time.format(sys.now()),
            "error": {
              "code": "training_failed",
              "message": "Training job failed",
              "type": "workflow_failure",
              "stack_trace": null
            },
            "context": {
              "execution_id": trainingExecutionName,
              "workflow_execution_id": sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),
              "attempt_number": 1,
              "config_context": {
                "output_path": output_path
              }
            }
          })}
        next: raise_training_error

    - raise_training_error:
        raise:
          message: ${"Training failed for job " + job_id}

    - log_training_success:
        assign:
          - trainingEndTime: '${time.format(sys.now())}'
          - trainingDurationSeconds: '${time.parse(trainingEndTime) - time.parse(trainingStartTime)}'
          - trainingTimeoutUsageRatio: '${trainingDurationSeconds / trainingTimeoutSeconds}'
          - trainingTimedOut: '${trainingDurationSeconds >= trainingTimeoutSeconds}'
        next: emit_training_metrics_complete

    - emit_training_metrics_complete:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "complete"
            workflow: "training-pipeline"
            job: ${trainingJobName}
            scene_id: ${scene_id}
            timeout_seconds: ${trainingTimeoutSeconds}
            duration_seconds: ${trainingDurationSeconds}
            timeout_usage_ratio: ${trainingTimeoutUsageRatio}
            timed_out: ${trainingTimedOut}
            status: "SUCCEEDED"
            start_time: ${trainingStartTime}
            end_time: ${trainingEndTime}
          severity: "INFO"
        next: log_training_success_message

    - log_training_success_message:
        call: sys.log
        args:
          text: ${"Training job completed successfully for " + job_id}
          severity: INFO
        next: parse_training_output

    # Extract training results (model path)
    - parse_training_output:
        assign:
          - model_path: ${text.replace_all(default(training_status.body.status.logUri, ""), "gs://" + bucket + "/", "")}
        next: return_success

    # Return success
    - return_success:
        return:
          status: "success"
          job_id: ${job_id}
          scene_id: ${scene_id}
          episode_count: ${episode_count}
          model_path: ${model_path}
          training_execution_id: ${training_job_execution.name}
