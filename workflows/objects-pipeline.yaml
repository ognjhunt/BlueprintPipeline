# objects-pipeline.yaml
#
# Google Cloud Workflows pipeline that:
#   1. Triggers on layout output (scene_layout.json)
#   2. Runs objects-job to extract object metadata
#   3. Writes outputs under scenes/{sceneId}/objects
#
# Trigger:
#   Type: Cloud Storage object finalized (Eventarc)
#   Bucket/prefix filter: bucket=${BUCKET}, object name matches scenes/*/layout/scene_layout.json
#
# Eventarc setup example:
#   gcloud eventarc triggers create objects-trigger \
#     --location=us-central1 \
#     --service-account="${WORKFLOW_SA}@${PROJECT_ID}.iam.gserviceaccount.com" \
#     --destination-workflow=objects-pipeline \
#     --destination-workflow-location=us-central1 \
#     --event-filters="type=google.cloud.storage.object.v1.finalized" \
#     --event-filters="bucket=${BUCKET}" \
#     --event-data-content-type="application/json"

main:
  params: [event]
  steps:
    - log_event:
        call: sys.log
        args:
          text: ${event}
          severity: "INFO"
        next: extract

    - extract:
        assign:
          - bucket: ${event.data.bucket}
          - object: ${event.data.name}
          - projectId: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - primaryRegion: ${default(sys.get_env("PRIMARY_WORKFLOW_REGION"), "us-central1")}
          - secondaryRegion: ${default(sys.get_env("SECONDARY_WORKFLOW_REGION"), "us-east1")}
          - region: ${primaryRegion}
        next: filter_layout_json

    # Only handle scenes/.../layout/scene_layout.json
    - filter_layout_json:
        switch:
          - condition: ${text.match_regex(object, "^scenes/.+/layout/scene_layout\\.json$")}
            next: derive
        next: skip

    - derive:
        assign:
          - parts: ${text.split(object, "/")}
          - sceneId: ${parts[1]}
          - da3Prefix: ${"scenes/" + sceneId + "/da3"}
          - segDatasetPrefix: ${"scenes/" + sceneId + "/seg/dataset"}
          - layoutPrefix: ${"scenes/" + sceneId + "/layout"}
          - objectsPrefix: ${"scenes/" + sceneId + "/objects"}
          - jobName: "objects-job"
          # Explicitly set to the Cloud Run default for consistency/metrics.
          - objectsTimeoutSeconds: 3600
          - objectsFailedMarker: ${"scenes/" + sceneId + "/objects/.failed"}
          - objectsLegacyFailedMarker: ${"scenes/" + sceneId + "/objects/.objects_failed"}
        next: init_region_health

    - init_region_health:
        assign:
          - primaryHealthy: false
          - secondaryHealthy: false
        next: check_primary_region

    - check_primary_region:
        try:
          call: googleapis.run.v1.projects.locations.jobs.get
          args:
            name: ${"projects/" + projectId + "/locations/" + primaryRegion + "/jobs/" + jobName}
          result: primaryJob
        next: mark_primary_healthy
        except:
          as: primaryError
          steps:
            - log_primary_unhealthy:
                call: sys.log
                args:
                  text: '${"Primary region " + primaryRegion + " job " + jobName + " unavailable: " + primaryError.message}'
                  severity: "WARNING"
            - check_secondary_region:
                try:
                  call: googleapis.run.v1.projects.locations.jobs.get
                  args:
                    name: ${"projects/" + projectId + "/locations/" + secondaryRegion + "/jobs/" + jobName}
                  result: secondaryJob
                next: mark_secondary_healthy
                except:
                  as: secondaryError
                  steps:
                    - log_secondary_unhealthy:
                        call: sys.log
                        args:
                          text: '${"Secondary region " + secondaryRegion + " job " + jobName + " unavailable: " + secondaryError.message}'
                          severity: "ERROR"
                    - select_region

    - mark_primary_healthy:
        assign:
          - primaryHealthy: true
        next: select_region

    - mark_secondary_healthy:
        assign:
          - secondaryHealthy: true
        next: select_region

    - select_region:
        switch:
          - condition: ${primaryHealthy}
            next: use_primary_region
          - condition: ${secondaryHealthy}
            next: use_secondary_region
        next: use_primary_region

    - use_primary_region:
        assign:
          - region: ${primaryRegion}
        next: check_if_done

    - use_secondary_region:
        assign:
          - region: ${secondaryRegion}
        next: check_if_done

    # Check if the output already exists to avoid duplicate processing
    - check_if_done:
        try:
          steps:
            - read_layout:
                call: http.get
                args:
                  url: ${"https://storage.googleapis.com/storage/v1/b/" + bucket + "/o/" + text.url_encode(object) + "?alt=media"}
                  auth:
                    type: OAuth2
                result: layoutResponse
            - parse_layout:
                assign:
                  - layoutData: ${json.decode(text.decode(layoutResponse.body))}
            - check_has_objects:
                switch:
                  - condition: ${len(layoutData.objects) > 0}
                    return: ${"objects-job already completed for " + object + " (objects already present)"}
        except:
          as: e
          steps:
            - log_check_error:
                call: sys.log
                args:
                  text: '${"Failed to check if objects already exist: " + e.message + ". Proceeding with job."}'
                  severity: "WARNING"
        next: init_objects_metrics

    - init_objects_metrics:
        assign:
          - objectsStartTime: '${time.format(sys.now())}'
        next: emit_objects_metrics_start

    - emit_objects_metrics_start:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "start"
            workflow: "objects-pipeline"
            job: ${jobName}
            scene_id: ${sceneId}
            timeout_seconds: ${objectsTimeoutSeconds}
            duration_seconds: 0
            timeout_usage_ratio: 0
            timed_out: false
            status: "STARTED"
            start_time: ${objectsStartTime}
          severity: "INFO"
        next: run_objects_job

    - run_objects_job:
        try:
          call: googleapis.run.v2.projects.locations.jobs.run
          args:
            name: ${"projects/" + projectId + "/locations/" + region + "/jobs/" + jobName}
            body:
              overrides:
                containerOverrides:
                  - env:
                      - name: BUCKET
                        value: ${bucket}
                      - name: SCENE_ID
                        value: ${sceneId}
                      - name: DA3_PREFIX
                        value: ${da3Prefix}
                      - name: SEG_DATASET_PREFIX
                        value: ${segDatasetPrefix}
                      - name: LAYOUT_PREFIX
                        value: ${layoutPrefix}
              timeout: '${string(objectsTimeoutSeconds) + "s"}'
          result: objectsExec
        retry:
          predicate: ${http.default_retry_predicate}
          max_retries: 5  # default per policy_configs/retry_policy.yaml
          backoff:
            initial_delay: 1
            max_delay: 60
            multiplier: 2
        except:
          as: e
          steps:
            - log_objects_retry_exhausted:
                call: sys.log
                args:
                  data:
                    bp_metric: "job_retry_exhausted"
                    workflow: "objects-pipeline"
                    job: ${jobName}
                    scene_id: ${sceneId}
                    retry_max: 5
                    error: ${e.message}
                  severity: "ERROR"
            - capture_objects_failure_metrics:
                assign:
                  - objectsEndTime: '${time.format(sys.now())}'
                  - objectsDurationSeconds: '${time.parse(objectsEndTime) - time.parse(objectsStartTime)}'
                  - objectsTimeoutUsageRatio: '${objectsDurationSeconds / objectsTimeoutSeconds}'
                  - objectsTimedOut: '${objectsDurationSeconds >= objectsTimeoutSeconds}'
            - emit_objects_metrics_failed:
                call: sys.log
                args:
                  data:
                    bp_metric: "job_invocation"
                    event: "complete"
                    workflow: "objects-pipeline"
                    job: ${jobName}
                    scene_id: ${sceneId}
                    timeout_seconds: ${objectsTimeoutSeconds}
                    duration_seconds: ${objectsDurationSeconds}
                    timeout_usage_ratio: ${objectsTimeoutUsageRatio}
                    timed_out: ${objectsTimedOut}
                    status: "FAILED"
                    start_time: ${objectsStartTime}
                    end_time: ${objectsEndTime}
                  severity: "ERROR"
            - log_job_error:
                call: sys.log
                args:
                  text: ${"ERROR: Failed to start objects-job after retries: " + e.message}
                  severity: "ERROR"
            - list_objects_outputs_for_cleanup:
                call: googleapis.storage.v1.objects.list
                args:
                  bucket: ${bucket}
                  prefix: ${objectsPrefix + "/"}
                  maxResults: 1000
                result: objectsCleanupList
            - delete_objects_outputs_for_cleanup:
                for:
                  value: objectsCleanupItem
                  in: ${default(objectsCleanupList.items, [])}
                  steps:
                    - delete_objects_output:
                        call: googleapis.storage.v1.objects.delete
                        args:
                          bucket: ${bucket}
                          object: ${objectsCleanupItem.name}
            - write_failure_marker:
                call: googleapis.storage.v1.objects.insert
                args:
                  bucket: ${bucket}
                  name: ${objectsFailedMarker}
                  uploadType: "media"
                  body: ${json.encode({
                    "scene_id": sceneId,
                    "job_name": jobName,
                    "status": "failed",
                    "timestamp": time.format(sys.now()),
                    "error": {
                      "code": "objects_job_start_failed",
                      "message": e.message,
                      "type": "workflow_failure",
                      "stack_trace": null
                    },
                    "context": {
                      "execution_id": "unknown",
                      "workflow_execution_id": sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),
                      "attempt_number": 1,
                      "config_context": {
                        "da3_prefix": da3Prefix,
                        "seg_dataset_prefix": segDatasetPrefix,
                        "layout_prefix": layoutPrefix
                      }
                    },
                    "input_params": {
                      "scene_id": sceneId,
                      "bucket": bucket
                    }
                  })}
            - write_legacy_failure_marker:
                call: googleapis.storage.v1.objects.insert
                args:
                  bucket: ${bucket}
                  name: ${objectsLegacyFailedMarker}
                  uploadType: "media"
                  body: ${"{\\"scene_id\\": \\"" + sceneId + "\\", \\"status\\": \\"failed\\", \\"error\\": \\"" + e.message + "\\"}" }
            - raise_objects_job_error:
                raise: ${e}
        next: set_objects_execution_name

    - set_objects_execution_name:
        assign:
          - objectsExecutionName: '${if(objectsExec.metadata != null and objectsExec.metadata.name != null, objectsExec.metadata.name, objectsExec.name)}'
        next: wait_for_objects

    - wait_for_objects:
        call: googleapis.run.v2.projects.locations.jobs.executions.get
        args:
          name: ${objectsExecutionName}
        result: objectsStatus
        next: check_objects_status

    - check_objects_status:
        assign:
          - objectsState: '${if(objectsStatus.state != null, objectsStatus.state, if(objectsStatus.status != null, objectsStatus.status.state, null))}'
          - objectsFailedCount: '${if(objectsStatus.failedCount != null, objectsStatus.failedCount, if(objectsStatus.status != null, objectsStatus.status.failedCount, null))}'
          - objectsSucceededCount: '${if(objectsStatus.succeededCount != null, objectsStatus.succeededCount, if(objectsStatus.status != null, objectsStatus.status.succeededCount, null))}'
        next: objects_status_switch

    - objects_status_switch:
        switch:
          - condition: '${objectsState == "FAILED" or (objectsFailedCount != null and objectsFailedCount > 0)}'
            next: objects_failed
          - condition: '${objectsState == "SUCCEEDED" or (objectsSucceededCount != null and objectsSucceededCount > 0)}'
            next: log_objects_complete
        next: wait_objects_poll

    - wait_objects_poll:
        call: sys.sleep
        args:
          seconds: 10
        next: wait_for_objects

    - objects_failed:
        assign:
          - objectsEndTime: '${time.format(sys.now())}'
          - objectsDurationSeconds: '${time.parse(objectsEndTime) - time.parse(objectsStartTime)}'
          - objectsTimeoutUsageRatio: '${objectsDurationSeconds / objectsTimeoutSeconds}'
          - objectsTimedOut: '${objectsDurationSeconds >= objectsTimeoutSeconds}'
        next: emit_objects_metrics_failed

    - emit_objects_metrics_failed:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "complete"
            workflow: "objects-pipeline"
            job: ${jobName}
            scene_id: ${sceneId}
            timeout_seconds: ${objectsTimeoutSeconds}
            duration_seconds: ${objectsDurationSeconds}
            timeout_usage_ratio: ${objectsTimeoutUsageRatio}
            timed_out: ${objectsTimedOut}
            status: "FAILED"
            start_time: ${objectsStartTime}
            end_time: ${objectsEndTime}
          severity: "ERROR"
        next: log_objects_failed_message

    - log_objects_failed_message:
        call: sys.log
        args:
          text: ${"WARNING: Objects job failed for scene " + sceneId + " but continuing pipeline"}
          severity: "WARNING"
        next: read_objects_failure_marker

    - log_objects_complete:
        assign:
          - objectsEndTime: '${time.format(sys.now())}'
          - objectsDurationSeconds: '${time.parse(objectsEndTime) - time.parse(objectsStartTime)}'
          - objectsTimeoutUsageRatio: '${objectsDurationSeconds / objectsTimeoutSeconds}'
          - objectsTimedOut: '${objectsDurationSeconds >= objectsTimeoutSeconds}'
        next: emit_objects_metrics_complete

    - emit_objects_metrics_complete:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "complete"
            workflow: "objects-pipeline"
            job: ${jobName}
            scene_id: ${sceneId}
            timeout_seconds: ${objectsTimeoutSeconds}
            duration_seconds: ${objectsDurationSeconds}
            timeout_usage_ratio: ${objectsTimeoutUsageRatio}
            timed_out: ${objectsTimedOut}
            status: "SUCCEEDED"
            start_time: ${objectsStartTime}
            end_time: ${objectsEndTime}
          severity: "INFO"
        next: mark_objects_complete

    - read_objects_failure_marker:
        try:
          call: googleapis.storage.v1.objects.get
          args:
            bucket: ${bucket}
            object: ${objectsFailedMarker}
            alt: "media"
          result: objectsFailurePayload
        except:
          as: e
          steps:
            - handle_missing_objects_failure_marker:
                switch:
                  - condition: '${e.code == 404}'
                    next: write_objects_failure_marker
                next: log_objects_failure_marker_read_error
        next: log_objects_failure_marker_payload

    - log_objects_failure_marker_read_error:
        call: sys.log
        args:
          text: '${"Failed to read .failed marker for objects job in scene " + sceneId + ": " + e.message}'
          severity: "WARNING"
        next: write_objects_failure_marker

    - log_objects_failure_marker_payload:
        call: sys.log
        args:
          text: '${"Objects failure marker payload for scene " + sceneId + ": " + objectsFailurePayload}'
          severity: "ERROR"
        next: mark_objects_complete

    - write_objects_failure_marker:
        try:
          call: googleapis.storage.v1.objects.insert
          args:
            bucket: ${bucket}
            name: ${objectsFailedMarker}
            uploadType: "media"
            body: ${json.encode({
              "scene_id": sceneId,
              "job_name": jobName,
              "status": "failed",
              "timestamp": time.format(sys.now()),
              "error": {
                "code": "objects_failed",
                "message": "Objects job failed",
                "type": "workflow_failure",
                "stack_trace": null
              },
              "context": {
                "execution_id": objectsExecutionName,
                "workflow_execution_id": sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),
                "attempt_number": 1,
                "config_context": {
                  "da3_prefix": da3Prefix,
                  "seg_dataset_prefix": segDatasetPrefix,
                  "layout_prefix": layoutPrefix
                }
              },
              "input_params": {
                "scene_id": sceneId,
                "bucket": bucket
              }
            })}
          result: objectsFailureMarkerResult
        except:
          as: e
          steps:
            - log_objects_marker_write_error:
                call: sys.log
                args:
                  text: '${"Failed to write .failed marker for objects job: " + e.message}'
                  severity: "WARNING"
        next: write_objects_legacy_failure_marker

    - write_objects_legacy_failure_marker:
        try:
          call: googleapis.storage.v1.objects.insert
          args:
            bucket: ${bucket}
            name: ${objectsLegacyFailedMarker}
            uploadType: "media"
            body: ${"{\\"scene_id\\": \\"" + sceneId + "\\", \\"status\\": \\"failed\\", \\"timestamp\\": \\"" + time.format(sys.now()) + "\\"}" }
          result: objectsLegacyFailureMarkerResult
        except:
          as: e
          steps:
            - log_objects_legacy_marker_write_error:
                call: sys.log
                args:
                  text: '${"Failed to write legacy objects failure marker: " + e.message}'
                  severity: "WARNING"
        next: mark_objects_complete

    - mark_objects_complete:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${bucket}
          name: ${"scenes/" + sceneId + "/objects/.objects_complete"}
          uploadType: "media"
          body: ${"{\\"scene_id\\": \\"" + sceneId + "\\", \\"status\\": \\"completed\\", \\"timestamp\\": \\"" + time.format(sys.now()) + "\\"}" }
        result: markerResult
        next: done

    - done:
        return: ${"objects-job started for " + object}

    - skip:
        return: ${"objects skip " + object}
