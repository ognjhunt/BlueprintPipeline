# scale-pipeline.yaml
#
# Google Cloud Workflows pipeline that:
#   1. Triggers on layout output (scene_layout.json)
#   2. Runs scale-job to normalize scene layout scale
#   3. Writes scene_layout_scaled.done marker
#
# Trigger:
#   Type: Cloud Storage object finalized (Eventarc)
#   Bucket/prefix filter: bucket=${BUCKET}, object name matches scenes/*/layout/scene_layout.json
#
# Eventarc setup example:
#   gcloud eventarc triggers create scale-trigger \
#     --location=us-central1 \
#     --service-account="${WORKFLOW_SA}@${PROJECT_ID}.iam.gserviceaccount.com" \
#     --destination-workflow=scale-pipeline \
#     --destination-workflow-location=us-central1 \
#     --event-filters="type=google.cloud.storage.object.v1.finalized" \
#     --event-filters="bucket=${BUCKET}" \
#     --event-data-content-type="application/json"

main:
  params: [event]
  steps:
    - log_event:
        call: sys.log
        args:
          text: ${event}
          severity: "INFO"
        next: extract

    - extract:
        assign:
          - bucket: ${event.data.bucket}
          - object: ${event.data.name}
          - projectId: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - region: ${default(sys.get_env("WORKFLOW_REGION"), "us-central1")}
        next: filter_layout_json

    # Only handle scenes/.../layout/scene_layout.json
    - filter_layout_json:
        switch:
          - condition: ${text.match_regex(object, "^scenes/.+/layout/scene_layout\\.json$")}
            next: derive
        next: skip

    - derive:
        assign:
          - parts: ${text.split(object, "/")}
          - sceneId: ${parts[1]}
          - layoutPrefix: ${"scenes/" + sceneId + "/layout"}
          - jobName: "scale-job"
          # Explicitly set to the Cloud Run default for consistency/metrics.
          - scaleTimeoutSeconds: 3600
          - doneMarkerPath: ${"scenes/" + sceneId + "/layout/scene_layout_scaled.done"}
          - scaleFailedMarker: ${"scenes/" + sceneId + "/layout/.failed"}
          - scaleLegacyFailedMarker: ${"scenes/" + sceneId + "/layout/.scale_failed"}
        next: check_if_done

    # Check if the output already exists to avoid duplicate processing
    - check_if_done:
        try:
          steps:
            - get_done_marker:
                call: googleapis.storage.v1.objects.get
                args:
                  bucket: ${bucket}
                  object: ${doneMarkerPath}
                result: doneMarkerObject
            - skip_already_done:
                return: ${"scale-job already completed for " + object + " (found done marker)"}
        except:
          as: e
          steps:
            - check_not_found:
                switch:
                  - condition: ${e.code == 404}
                    next: run_scale_job
            - raise_error:
                raise: ${e}
        next: init_scale_metrics

    - init_scale_metrics:
        assign:
          - scaleStartTime: '${time.format(sys.now())}'
        next: emit_scale_metrics_start

    - emit_scale_metrics_start:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "start"
            workflow: "scale-pipeline"
            job: ${jobName}
            scene_id: ${sceneId}
            timeout_seconds: ${scaleTimeoutSeconds}
            duration_seconds: 0
            timeout_usage_ratio: 0
            timed_out: false
            status: "STARTED"
            start_time: ${scaleStartTime}
          severity: "INFO"
        next: run_scale_job

    - run_scale_job:
        try:
          call: googleapis.run.v2.projects.locations.jobs.run
          args:
            name: ${"projects/" + projectId + "/locations/" + region + "/jobs/" + jobName}
            body:
              overrides:
                containerOverrides:
                  - env:
                      - name: BUCKET
                        value: ${bucket}
                      - name: SCENE_ID
                        value: ${sceneId}
                      - name: LAYOUT_PREFIX
                        value: ${layoutPrefix}
              timeout: '${string(scaleTimeoutSeconds) + "s"}'
          result: scaleExec
        retry:
          predicate: ${http.default_retry_predicate}
          max_retries: 5  # default per policy_configs/retry_policy.yaml
          backoff:
            initial_delay: 1
            max_delay: 60
            multiplier: 2
        except:
          as: e
          steps:
            - log_scale_retry_exhausted:
                call: sys.log
                args:
                  data:
                    bp_metric: "job_retry_exhausted"
                    workflow: "scale-pipeline"
                    job: ${jobName}
                    scene_id: ${sceneId}
                    retry_max: 5
                    error: ${e.message}
                  severity: "ERROR"
            - capture_scale_failure_metrics:
                assign:
                  - scaleEndTime: '${time.format(sys.now())}'
                  - scaleDurationSeconds: '${time.parse(scaleEndTime) - time.parse(scaleStartTime)}'
                  - scaleTimeoutUsageRatio: '${scaleDurationSeconds / scaleTimeoutSeconds}'
                  - scaleTimedOut: '${scaleDurationSeconds >= scaleTimeoutSeconds}'
            - emit_scale_metrics_failed:
                call: sys.log
                args:
                  data:
                    bp_metric: "job_invocation"
                    event: "complete"
                    workflow: "scale-pipeline"
                    job: ${jobName}
                    scene_id: ${sceneId}
                    timeout_seconds: ${scaleTimeoutSeconds}
                    duration_seconds: ${scaleDurationSeconds}
                    timeout_usage_ratio: ${scaleTimeoutUsageRatio}
                    timed_out: ${scaleTimedOut}
                    status: "FAILED"
                    start_time: ${scaleStartTime}
                    end_time: ${scaleEndTime}
                  severity: "ERROR"
            - log_job_error:
                call: sys.log
                args:
                  text: ${"ERROR: Failed to start scale-job after retries: " + e.message}
                  severity: "ERROR"
            - list_scale_outputs_for_cleanup:
                call: googleapis.storage.v1.objects.list
                args:
                  bucket: ${bucket}
                  prefix: ${layoutPrefix + "/"}
                  maxResults: 1000
                result: scaleCleanupList
            - delete_scale_outputs_for_cleanup:
                for:
                  value: scaleCleanupItem
                  in: ${default(scaleCleanupList.items, [])}
                  steps:
                    - delete_scale_output:
                        call: googleapis.storage.v1.objects.delete
                        args:
                          bucket: ${bucket}
                          object: ${scaleCleanupItem.name}
            - write_failure_marker:
                call: googleapis.storage.v1.objects.insert
                args:
                  bucket: ${bucket}
                  name: ${scaleFailedMarker}
                  uploadType: "media"
                  body: ${json.encode({
                    "scene_id": sceneId,
                    "job_name": jobName,
                    "status": "failed",
                    "timestamp": time.format(sys.now()),
                    "error": {
                      "code": "scale_job_start_failed",
                      "message": e.message,
                      "type": "workflow_failure",
                      "stack_trace": null
                    },
                    "context": {
                      "execution_id": "unknown",
                      "workflow_execution_id": sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),
                      "attempt_number": 1,
                      "config_context": {
                        "layout_prefix": layoutPrefix
                      }
                    },
                    "input_params": {
                      "scene_id": sceneId,
                      "bucket": bucket
                    }
                  })}
            - write_legacy_failure_marker:
                call: googleapis.storage.v1.objects.insert
                args:
                  bucket: ${bucket}
                  name: ${scaleLegacyFailedMarker}
                  uploadType: "media"
                  body: ${"{\\"scene_id\\": \\"" + sceneId + "\\", \\"status\\": \\"failed\\", \\"error\\": \\"" + e.message + "\\"}" }
            - raise_scale_job_error:
                raise: ${e}
        next: log_scale_metrics_complete

    - log_scale_metrics_complete:
        assign:
          - scaleEndTime: '${time.format(sys.now())}'
          - scaleDurationSeconds: '${time.parse(scaleEndTime) - time.parse(scaleStartTime)}'
          - scaleTimeoutUsageRatio: '${scaleDurationSeconds / scaleTimeoutSeconds}'
          - scaleTimedOut: '${scaleDurationSeconds >= scaleTimeoutSeconds}'
        next: emit_scale_metrics_complete

    - emit_scale_metrics_complete:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "complete"
            workflow: "scale-pipeline"
            job: ${jobName}
            scene_id: ${sceneId}
            timeout_seconds: ${scaleTimeoutSeconds}
            duration_seconds: ${scaleDurationSeconds}
            timeout_usage_ratio: ${scaleTimeoutUsageRatio}
            timed_out: ${scaleTimedOut}
            status: "SUBMITTED"
            start_time: ${scaleStartTime}
            end_time: ${scaleEndTime}
          severity: "INFO"
        next: mark_scale_complete

    - mark_scale_complete:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${bucket}
          name: ${"scenes/" + sceneId + "/layout/.scale_complete"}
          uploadType: "media"
          body: ${"{\\"scene_id\\": \\"" + sceneId + "\\", \\"status\\": \\"processing\\", \\"timestamp\\": \\"" + time.format(sys.now()) + "\\"}" }
        result: markerResult
        next: write_assets_ready_marker

    # Write .assets_ready marker to signal that layout processing is complete
    # This triggers downstream pipelines (regen3d-pipeline, etc)
    - write_assets_ready_marker:
        try:
          call: googleapis.storage.v1.objects.insert
          args:
            bucket: ${bucket}
            name: ${"scenes/" + sceneId + "/assets/.assets_ready"}
            uploadType: "media"
            body: ${"{\\"scene_id\\": \\"" + sceneId + "\\", \\"status\\": \\"ready\\", \\"timestamp\\": \\"" + time.format(sys.now()) + "\\"}" }
          result: assetsReadyMarker
        except:
          as: e
          steps:
            - log_assets_ready_error:
                call: sys.log
                args:
                  text: ${"WARNING: Failed to write .assets_ready marker: " + e.message}
                  severity: "WARNING"
        next: done

    - done:
        return: ${"scale-job started for " + object}

    - skip:
        return: ${"scale skip " + object}
