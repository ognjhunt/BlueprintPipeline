# =============================================================================
# Genie Sim Import Pipeline
# =============================================================================
#
# Imports generated episodes from Genie Sim 3.0 back into BlueprintPipeline.
#
# Trigger Methods:
# 1. Manual trigger via Eventarc:
#    gcloud eventarc triggers create geniesim-import-trigger \
#      --destination-workflow=genie-sim-import-pipeline \
#      --event-filters="type=manual.geniesim.job.completed"
#
# 2. Scheduled polling (fallback):
#    Cloud Scheduler → Pub/Sub → Eventarc → Workflow
#
# Pipeline Flow:
#   Genie Sim Job Complete → Import Job → Episode Validation → Delivery Trigger → Training Pipeline
#
# Delivery Trigger:
#   After a successful import, this workflow triggers the dataset delivery flow
#   (via Eventarc) with the scene/job metadata and manifest location so operators
#   can monitor delivery separately from training.
#

main:
  params: [event]
  steps:
    - init:
        assign:
          - project_id: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - region: ${default(sys.get_env("WORKFLOW_REGION"), "us-central1")}
          - bucket: ${default(sys.get_env("PRIMARY_BUCKET"), sys.get_env("GCS_BUCKET", "blueprintpipeline-data"))}
          - firebase_bucket: ${default(sys.get_env("FIREBASE_STORAGE_BUCKET"), "")}
          - firebase_service_account_json: ${default(sys.get_env("FIREBASE_SERVICE_ACCOUNT_JSON"), "")}
          - firebase_service_account_path: ${default(sys.get_env("FIREBASE_SERVICE_ACCOUNT_PATH"), "/secrets/firebase/service-account.json")}
          - firebase_upload_prefix: ${default(sys.get_env("FIREBASE_UPLOAD_PREFIX"), "datasets")}
          - realtime_stream_enabled: ${default(sys.get_env("ENABLE_REALTIME_STREAMING"), "0")}
          - realtime_stream_endpoint: ${default(sys.get_env("REALTIME_STREAM_ENDPOINT"), "")}
          - realtime_stream_api_key: ${default(sys.get_env("REALTIME_STREAM_API_KEY"), "")}
          - realtime_stream_batch_size: ${default(sys.get_env("REALTIME_STREAM_BATCH_SIZE"), "25")}
          - circuitBreakerThreshold: ${int(default(sys.get_env("GENIESIM_CIRCUIT_BREAKER_THRESHOLD"), "3"))}
          - artifacts_by_robot: null

    - extract_event:
        switch:
          # Direct workflow payload (e.g., local runner or poller)
          - condition: ${"job_id" in event}
            assign:
              - job_id: ${event.job_id}
              - status: ${event.status}
              - scene_id: ${default(event.scene_id, "unknown")}
              - local_episodes_prefix: ${default(event.local_episodes_prefix, null)}
              - job_metadata_path: ${default(event.job_metadata_path, null)}

          # Eventarc event (wrapped)
          - condition: ${"data" in event and "job_id" in event.data}
            assign:
              - job_id: ${event.data.job_id}
              - status: ${event.data.status}
              - scene_id: ${default(event.data.scene_id, "unknown")}
              - local_episodes_prefix: ${default(event.data.local_episodes_prefix, null)}
              - job_metadata_path: ${default(event.data.job_metadata_path, null)}

          # Default/manual trigger
          - condition: true
            assign:
              - job_id: ${event.job_id}
              - status: ${default(event.status, "completed")}
              - scene_id: ${default(event.scene_id, "unknown")}
              - local_episodes_prefix: ${default(event.local_episodes_prefix, null)}
              - job_metadata_path: ${default(event.job_metadata_path, null)}
        next: init_circuit_breaker

    - init_circuit_breaker:
        assign:
          - circuitBreakerObject: ${scene_id != null and scene_id != "" and scene_id != "unknown" ? "scenes/" + scene_id + "/geniesim/.circuit_breaker.json" : null}
          - circuitBreakerFailureCount: 0
          - circuitBreakerLastFailureAt: null
          - circuitBreakerLastSuccessAt: null
        next: read_circuit_breaker_state

    - read_circuit_breaker_state:
        switch:
          - condition: ${circuitBreakerObject == null}
            next: log_circuit_breaker_skip
        next: fetch_circuit_breaker_state

    - log_circuit_breaker_skip:
        call: sys.log
        args:
          text: "Skipping circuit breaker check; scene_id is missing."
          severity: "WARNING"
        next: resolve_job_id

    - fetch_circuit_breaker_state:
        try:
          call: googleapis.storage.v1.objects.get
          args:
            bucket: ${bucket}
            object: ${circuitBreakerObject}
            alt: "media"
          result: circuitBreakerRaw
        except:
          as: circuitBreakerError
          steps:
            - handle_missing_circuit_breaker_state:
                switch:
                  - condition: ${circuitBreakerError.code == 404}
                    next: set_empty_circuit_breaker_state
                next: log_circuit_breaker_read_error
            - set_empty_circuit_breaker_state:
                assign:
                  - circuitBreakerRaw: "{}"
                next: parse_circuit_breaker_state
            - log_circuit_breaker_read_error:
                call: sys.log
                args:
                  text: ${"Unable to read circuit breaker state for scene " + scene_id + ": " + circuitBreakerError.message}
                  severity: "WARNING"
                next: set_circuit_breaker_state_after_error
            - set_circuit_breaker_state_after_error:
                assign:
                  - circuitBreakerRaw: "{}"
                next: log_circuit_breaker_state
        next: parse_circuit_breaker_state

    - parse_circuit_breaker_state:
        assign:
          - circuitBreakerState: ${json.decode(circuitBreakerRaw)}
          - circuitBreakerFailureCount: ${default(map.get(circuitBreakerState, "failure_count"), 0)}
          - circuitBreakerLastFailureAt: ${default(map.get(circuitBreakerState, "last_failure_at"), null)}
          - circuitBreakerLastSuccessAt: ${default(map.get(circuitBreakerState, "last_success_at"), null)}
        next: log_circuit_breaker_state

    - log_circuit_breaker_state:
        call: sys.log
        args:
          text: ${"Circuit breaker state for scene " + scene_id + ": failures=" + string(circuitBreakerFailureCount) + ", threshold=" + string(circuitBreakerThreshold) + ", object=" + circuitBreakerObject}
          severity: "INFO"
        next: check_circuit_breaker_threshold

    - check_circuit_breaker_threshold:
        switch:
          - condition: ${circuitBreakerFailureCount >= circuitBreakerThreshold}
            next: circuit_breaker_open
        next: resolve_job_id

    - circuit_breaker_open:
        call: sys.log
        args:
          text: ${"Circuit breaker open for scene " + scene_id + " (failures=" + string(circuitBreakerFailureCount) + ", threshold=" + string(circuitBreakerThreshold) + "). Skipping Genie Sim import."}
          severity: "ERROR"
        next: stop_for_circuit_breaker

    - stop_for_circuit_breaker:
        return: ${"Circuit breaker open for scene " + scene_id + ". Skipping Genie Sim import."}

    - resolve_job_id:
        switch:
          - condition: ${job_id == null or job_id == ""}
            next: fetch_job_metadata
        next: validate_inputs

    - fetch_job_metadata:
        switch:
          - condition: ${scene_id == null or scene_id == "" or scene_id == "unknown"}
            next: missing_job_id
        next: read_job_metadata

    - read_job_metadata:
        try:
          call: googleapis.storage.v1.objects.get
          args:
            bucket: ${bucket}
            object: '${"scenes/" + scene_id + "/geniesim/job.json"}'
            alt: "media"
          result: jobMetadataRaw
        except:
          as: e
          steps:
            - log_missing_job_metadata:
                call: sys.log
                args:
                  text: ${"Failed to load stored job metadata for scene " + scene_id + ": " + e.message}
                  severity: "ERROR"
            - raise_missing_job_metadata:
                raise: ${e}
        next: parse_job_metadata

    - parse_job_metadata:
        assign:
          - jobMetadata: ${json.decode(jobMetadataRaw)}
          - job_id: ${jobMetadata.job_id}
        next: validate_inputs

    - missing_job_id:
        raise:
          message: "job_id is required and could not be resolved from scene metadata"

    - validate_inputs:
        switch:
          - condition: ${job_id == null or job_id == ""}
            raise:
              message: "job_id is required"
        next: set_job_metadata_path

    - set_job_metadata_path:
        switch:
          - condition: ${job_metadata_path == null and scene_id != null and scene_id != "" and scene_id != "unknown"}
            assign:
              - job_metadata_path: '${"scenes/" + scene_id + "/geniesim/job.json"}'
        next: init_import_idempotency

    - init_import_idempotency:
        switch:
          - condition: ${scene_id == null or scene_id == "" or scene_id == "unknown"}
            next: maybe_read_job_metadata_for_import
        assign:
          - importIdempotencyBasePath: ${"scenes/" + scene_id + "/geniesim/idempotency/import"}
          - importIdempotencyObject: ${importIdempotencyBasePath + "/" + job_id + ".json"}
        next: check_import_idempotency

    - check_import_idempotency:
        try:
          call: googleapis.storage.v1.objects.get
          args:
            bucket: ${bucket}
            object: ${importIdempotencyObject}
            alt: "media"
          result: importIdempotencyRaw
        except:
          as: importIdempotencyError
          steps:
            - handle_import_idempotency_missing:
                switch:
                  - condition: ${importIdempotencyError.code == 404}
                    next: maybe_read_job_metadata_for_import
                next: raise_import_idempotency_error
            - raise_import_idempotency_error:
                raise: ${importIdempotencyError}
        next: parse_import_idempotency_marker

    - parse_import_idempotency_marker:
        assign:
          - importIdempotencyMarker: ${json.decode(importIdempotencyRaw)}
          - importIdempotencyStatus: ${default(map.get(importIdempotencyMarker, "status"), "unknown")}
        next: evaluate_import_idempotency_marker

    - evaluate_import_idempotency_marker:
        switch:
          - condition: ${importIdempotencyStatus == "completed"}
            next: return_idempotent_import_skip
        next: maybe_read_job_metadata_for_import

    - return_idempotent_import_skip:
        call: sys.log
        args:
          text: ${"Idempotency marker present for import job " + job_id + " (scene: " + scene_id + "). Skipping import."}
          severity: "INFO"
        next: return_import_idempotency_skip

    - return_import_idempotency_skip:
        return:
          status: "skipped"
          job_id: ${job_id}
          scene_id: ${scene_id}
          reason: "Idempotency marker indicates import already completed."

    - maybe_read_job_metadata_for_import:
        switch:
          - condition: ${job_metadata_path != null and local_episodes_prefix == null}
            next: read_job_metadata_for_import
        next: log_start

    - read_job_metadata_for_import:
        try:
          call: googleapis.storage.v1.objects.get
          args:
            bucket: ${bucket}
            object: ${job_metadata_path}
            alt: "media"
          result: importJobMetadataRaw
        except:
          as: e
          steps:
            - log_import_job_metadata_missing:
                call: sys.log
                args:
                  text: ${"Failed to load import job metadata from " + job_metadata_path + ": " + e.message}
                  severity: "WARNING"
            - set_empty_import_job_metadata:
                assign:
                  - importJobMetadataRaw: "{}"
        next: parse_import_job_metadata

    - parse_import_job_metadata:
        assign:
          - importJobMetadata: ${json.decode(importJobMetadataRaw)}
          - artifacts_by_robot: ${default(map.get(importJobMetadata, "artifacts_by_robot"), null)}
          - local_episodes_prefix: ${artifacts_by_robot == null ? default(map.get(map.get(importJobMetadata, "artifacts", {}), "episodes_prefix"), default(map.get(map.get(importJobMetadata, "artifacts", {}), "episodes_path"), local_episodes_prefix)) : local_episodes_prefix}
        next: init_quality_gate_preflight

    # Preflight quality gate check (downstream guardrail)
    - init_quality_gate_preflight:
        assign:
          - qualityGateReportObject: ${"scenes/" + scene_id + "/episode-generation-job/quality_gate_report.json"}
          - qualityGateReportPath: ${"gs://" + bucket + "/" + qualityGateReportObject}
          - qualityGateFailedMarker: ${"scenes/" + scene_id + "/geniesim/.failed"}
          - importOutputPrefix: ${default(local_episodes_prefix, "scenes/" + scene_id + "/episodes")}
          - importFailedMarker: ${default(local_episodes_prefix, "scenes/" + scene_id + "/episodes") + "/.failed"}
          - qualityCertPrefix: ${"scenes/" + scene_id + "/episodes/"}
          - qualityCertObject: null
          - qualityCertPageToken: null
        next: maybe_check_quality_gates

    - maybe_check_quality_gates:
        switch:
          - condition: ${scene_id == null or scene_id == "" or scene_id == "unknown"}
            next: log_quality_gate_skip
        next: read_quality_gate_report

    - log_quality_gate_skip:
        call: sys.log
        args:
          text: "Skipping quality gate preflight; scene_id is missing."
          severity: "WARNING"
        next: log_start

    - read_quality_gate_report:
        try:
          call: googleapis.storage.v1.objects.get
          args:
            bucket: ${bucket}
            object: ${qualityGateReportObject}
            alt: "media"
          result: qualityGateReportRaw
        except:
          as: e
          steps:
            - handle_quality_gate_report_missing:
                switch:
                  - condition: ${e.code == 404}
                    next: list_quality_certificates
                next: raise_quality_gate_report_error

    - raise_quality_gate_report_error:
        raise:
          message: ${"Failed to read quality gate report at " + qualityGateReportPath}

    - parse_quality_gate_report:
        assign:
          - qualityGateReport: ${json.decode(qualityGateReportRaw)}
        next: evaluate_quality_gate_report

    - evaluate_quality_gate_report:
        switch:
          - condition: ${"summary" in qualityGateReport and qualityGateReport.summary.can_proceed == true and qualityGateReport.summary.blocking_failures == 0}
            next: log_quality_gate_passed
        next: quality_gate_failed

    - list_quality_certificates:
        call: googleapis.storage.v1.objects.list
        args:
          bucket: ${bucket}
          prefix: ${qualityCertPrefix}
          maxResults: 1000
          pageToken: ${qualityCertPageToken}
        result: qualityCertList
        next: scan_quality_certificates

    - scan_quality_certificates:
        for:
          value: certItem
          in: ${default(qualityCertList.items, [])}
          steps:
            - check_quality_cert_match:
                switch:
                  - condition: ${qualityCertObject == null and text.match_regex(certItem.name, "quality_certificate\\.json$")}
                    assign:
                      - qualityCertObject: ${certItem.name}
        next: check_quality_cert_scan_results

    - check_quality_cert_scan_results:
        switch:
          - condition: ${qualityCertObject != null}
            next: read_quality_certificate
          - condition: ${qualityCertList.nextPageToken != null}
            assign:
              - qualityCertPageToken: ${qualityCertList.nextPageToken}
            next: list_quality_certificates
        next: quality_gate_missing_artifacts

    - read_quality_certificate:
        call: googleapis.storage.v1.objects.get
        args:
          bucket: ${bucket}
          object: ${qualityCertObject}
          alt: "media"
        result: qualityCertRaw
        next: parse_quality_certificate

    - parse_quality_certificate:
        assign:
          - qualityCertificate: ${json.decode(qualityCertRaw)}
        next: evaluate_quality_certificate

    - evaluate_quality_certificate:
        switch:
          - condition: ${qualityCertificate.validation_passed == true and qualityCertificate.recommended_use != "testing"}
            next: log_quality_gate_passed_with_certificate
        next: quality_gate_failed

    - log_quality_gate_passed_with_certificate:
        call: sys.log
        args:
          text: ${"Quality gate report missing; proceeding based on certificate " + qualityCertObject}
          severity: "WARNING"
        next: log_start

    - quality_gate_missing_artifacts:
        call: sys.log
        args:
          text: ${"Missing quality gate report and quality certificates for scene " + scene_id}
          severity: "ERROR"
        next: quality_gate_failed

    - log_quality_gate_passed:
        call: sys.log
        args:
          text: ${"Quality gate passed for scene " + scene_id}
          severity: "INFO"
        next: log_start

    - quality_gate_failed:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${bucket}
          name: ${qualityGateFailedMarker}
          uploadType: "media"
          body: ${json.encode({
            "scene_id": scene_id,
            "status": "failed",
            "timestamp": time.format(sys.now()),
            "error": {
              "code": "quality_gate_failed",
              "message": "Downstream quality gate preflight blocked import",
              "report_path": qualityGateReportPath
            }
          })}
        result: qualityGateFailedMarkerResult
        next: raise_quality_gate_failed

    - raise_quality_gate_failed:
        raise:
          message: ${"Quality gate failed; report: " + qualityGateReportPath}

    - log_start:
        call: sys.log
        args:
          text: ${"Starting Genie Sim import for job " + job_id + " (scene: " + scene_id + ")"}
          severity: INFO

    # Check if job completed successfully
    - check_status:
        switch:
          - condition: ${status != "completed"}
            steps:
              - log_skip:
                  call: sys.log
                  args:
                    text: ${"Job " + job_id + " status is '" + status + "', skipping import"}
                    severity: WARNING
              - return_skipped:
                  return:
                    status: "skipped"
                    job_id: ${job_id}
                    reason: ${"Job status is " + status}

    # Run import job
    - init_import_metrics:
        assign:
          - importStartTime: '${time.format(sys.now())}'
          - importTimeoutSeconds: 1800
        next: emit_import_metrics_start

    - emit_import_metrics_start:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "start"
            workflow: "genie-sim-import-pipeline"
            job: "genie-sim-import-job"
            scene_id: ${scene_id}
            job_id: ${job_id}
            timeout_seconds: ${importTimeoutSeconds}
            retry_max: 5
            start_time: ${importStartTime}
          severity: "INFO"
        next: run_import_job

    - run_import_job:
        try:
          call: googleapis.run.v2.projects.locations.jobs.run
          args:
            name: ${"projects/" + project_id + "/locations/" + region + "/jobs/genie-sim-import-job"}
            body:
              overrides:
                containerOverrides:
                  - name: "genie-sim-import"
                    env:
                      # Supported overrides via workflow payload:
                      #   min_quality_score, enable_validation, filter_low_quality, wait_for_completion
                      - name: BUCKET
                        value: ${bucket}
                      - name: GENIE_SIM_JOB_ID
                        value: ${job_id}
                      - name: SCENE_ID
                        value: ${scene_id}
                      - name: WAIT_FOR_COMPLETION
                        value: ${default(event.wait_for_completion, "true")}
                      - name: MIN_QUALITY_SCORE
                        value: ${default(event.min_quality_score, "0.85")}
                      - name: ENABLE_VALIDATION
                        value: ${default(event.enable_validation, "true")}
                      - name: FILTER_LOW_QUALITY
                        value: ${default(event.filter_low_quality, "true")}
                      - name: JOB_METADATA_PATH
                        value: ${default(job_metadata_path, "")}
                      - name: LOCAL_EPISODES_PREFIX
                        value: ${default(local_episodes_prefix, "")}
                      - name: ARTIFACTS_BY_ROBOT
                        value: ${artifacts_by_robot != null ? json.encode(artifacts_by_robot) : ""}
                      - name: ENABLE_FIREBASE_UPLOAD
                        value: "true"
                      - name: FIREBASE_STORAGE_BUCKET
                        value: ${firebase_bucket}
                      - name: FIREBASE_SERVICE_ACCOUNT_JSON
                        value: ${firebase_service_account_json}
                      - name: FIREBASE_SERVICE_ACCOUNT_PATH
                        value: ${firebase_service_account_path}
                      - name: FIREBASE_UPLOAD_PREFIX
                        value: ${firebase_upload_prefix}
                      - name: ENABLE_REALTIME_STREAMING
                        value: ${realtime_stream_enabled}
                      - name: REALTIME_STREAM_ENDPOINT
                        value: ${realtime_stream_endpoint}
                      - name: REALTIME_STREAM_API_KEY
                        value: ${realtime_stream_api_key}
                      - name: REALTIME_STREAM_BATCH_SIZE
                        value: ${realtime_stream_batch_size}
                timeout: "1800s"  # 30 minutes max
          result: import_job_execution
        retry:
          predicate: ${http.default_retry_predicate}
          max_retries: 5
          backoff:
            initial_delay: 1
            max_delay: 60
            multiplier: 2
        except:
          as: e
          steps:
            - log_import_retry_exhausted:
                call: sys.log
                args:
                  data:
                    bp_metric: "job_retry_exhausted"
                    workflow: "genie-sim-import-pipeline"
                    job: "genie-sim-import-job"
                    scene_id: ${scene_id}
                    job_id: ${job_id}
                    retry_max: 5
                    error: ${e.message}
                  severity: "ERROR"
            - log_import_job_error:
                call: sys.log
                args:
                  text: ${"Failed to start genie-sim-import job after retries: " + e.message}
                  severity: "ERROR"
            - list_import_outputs_for_cleanup:
                call: googleapis.storage.v1.objects.list
                args:
                  bucket: ${bucket}
                  prefix: ${importOutputPrefix + "/"}
                  maxResults: 1000
                result: importCleanupList
            - delete_import_outputs_for_cleanup:
                for:
                  value: importCleanupItem
                  in: ${default(importCleanupList.items, [])}
                  steps:
                    - delete_import_output:
                        call: googleapis.storage.v1.objects.delete
                        args:
                          bucket: ${bucket}
                          object: ${importCleanupItem.name}
            - write_import_failure_marker:
                call: googleapis.storage.v1.objects.insert
                args:
                  bucket: ${bucket}
                  name: ${importFailedMarker}
                  uploadType: "media"
                  body: ${json.encode({
                    "scene_id": scene_id,
                    "job_name": "genie-sim-import-job",
                    "status": "failed",
                    "timestamp": time.format(sys.now()),
                    "error": {
                      "code": "genie_sim_import_start_failed",
                      "message": e.message,
                      "type": "workflow_failure",
                      "stack_trace": null
                    },
                    "context": {
                      "workflow_execution_id": sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),
                      "attempt_number": 1,
                      "config_context": {
                        "output_prefix": importOutputPrefix
                      }
                    }
                  })}
            - update_circuit_breaker_on_import_start_failure:
                switch:
                  - condition: ${circuitBreakerObject == null}
                    next: raise_import_job_error
                next: increment_circuit_breaker_import_start_failure
            - increment_circuit_breaker_import_start_failure:
                assign:
                  - circuitBreakerFailureCount: ${circuitBreakerFailureCount + 1}
                next: write_circuit_breaker_import_start_failure
            - write_circuit_breaker_import_start_failure:
                call: googleapis.storage.v1.objects.insert
                args:
                  bucket: ${bucket}
                  name: ${circuitBreakerObject}
                  uploadType: "media"
                  body: ${json.encode({
                    "scene_id": scene_id,
                    "failure_count": circuitBreakerFailureCount,
                    "threshold": circuitBreakerThreshold,
                    "status": "open",
                    "updated_at": time.format(sys.now()),
                    "last_failure_at": time.format(sys.now()),
                    "last_failure_reason": "geniesim_import_start_failed",
                    "last_success_at": circuitBreakerLastSuccessAt
                  })}
                next: log_circuit_breaker_import_start_failure
            - log_circuit_breaker_import_start_failure:
                call: sys.log
                args:
                  text: ${"Circuit breaker failure count updated to " + string(circuitBreakerFailureCount) + " after import start failure for scene " + scene_id}
                  severity: "WARNING"
            - raise_import_job_error:
                raise: ${e}

    - set_import_execution_name:
        assign:
          - importExecutionName: '${if(import_job_execution.metadata != null and import_job_execution.metadata.name != null, import_job_execution.metadata.name, import_job_execution.name)}'
        next: wait_for_import

    - wait_for_import:
        call: googleapis.run.v2.projects.locations.jobs.executions.get
        args:
          name: ${importExecutionName}
        result: import_status
        next: check_import_status

    - check_import_status:
        assign:
          - importState: '${if(import_status.state != null, import_status.state, if(import_status.status != null, import_status.status.state, null))}'
          - importFailedCount: '${if(import_status.failedCount != null, import_status.failedCount, if(import_status.status != null, import_status.status.failedCount, null))}'
          - importSucceededCount: '${if(import_status.succeededCount != null, import_status.succeededCount, if(import_status.status != null, import_status.status.succeededCount, null))}'
        next: import_status_switch

    - import_status_switch:
        switch:
          - condition: '${importState == "FAILED" or (importFailedCount != null and importFailedCount > 0)}'
            next: check_import_success
          - condition: '${importState == "SUCCEEDED" or (importSucceededCount != null and importSucceededCount > 0)}'
            next: log_import_success
        next: wait_import_poll

    - wait_import_poll:
        call: sys.sleep
        args:
          seconds: 10
        next: wait_for_import

    - check_import_success:
        assign:
          - importEndTime: '${time.format(sys.now())}'
          - importDurationSeconds: '${time.parse(importEndTime) - time.parse(importStartTime)}'
          - importTimeoutUsageRatio: '${importDurationSeconds / importTimeoutSeconds}'
          - importTimedOut: '${importDurationSeconds >= importTimeoutSeconds}'
        next: emit_import_metrics_failed

    - emit_import_metrics_failed:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "complete"
            workflow: "genie-sim-import-pipeline"
            job: "genie-sim-import-job"
            scene_id: ${scene_id}
            job_id: ${job_id}
            timeout_seconds: ${importTimeoutSeconds}
            duration_seconds: ${importDurationSeconds}
            timeout_usage_ratio: ${importTimeoutUsageRatio}
            timed_out: ${importTimedOut}
            status: "FAILED"
            execution_name: ${importExecutionName}
            start_time: ${importStartTime}
            end_time: ${importEndTime}
          severity: "ERROR"
        next: log_import_failed_message

    - log_import_failed_message:
        call: sys.log
        args:
          text: ${"Import job failed for job " + job_id}
          severity: ERROR
        next: update_circuit_breaker_on_import_failure

    - update_circuit_breaker_on_import_failure:
        switch:
          - condition: ${circuitBreakerObject == null}
            next: raise_import_error
        next: increment_circuit_breaker_import_failure

    - increment_circuit_breaker_import_failure:
        assign:
          - circuitBreakerFailureCount: ${circuitBreakerFailureCount + 1}
        next: write_circuit_breaker_import_failure

    - write_circuit_breaker_import_failure:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${bucket}
          name: ${circuitBreakerObject}
          uploadType: "media"
          body: ${json.encode({
            "scene_id": scene_id,
            "failure_count": circuitBreakerFailureCount,
            "threshold": circuitBreakerThreshold,
            "status": "open",
            "updated_at": time.format(sys.now()),
            "last_failure_at": time.format(sys.now()),
            "last_failure_reason": "geniesim_import_failed",
            "last_success_at": circuitBreakerLastSuccessAt
          })}
        next: log_circuit_breaker_import_failure

    - log_circuit_breaker_import_failure:
        call: sys.log
        args:
          text: ${"Circuit breaker failure count updated to " + string(circuitBreakerFailureCount) + " after import failure for scene " + scene_id}
          severity: "WARNING"
        next: raise_import_error

    - raise_import_error:
        raise:
          message: ${"Import failed for job " + job_id}

    - log_import_success:
        assign:
          - importEndTime: '${time.format(sys.now())}'
          - importDurationSeconds: '${time.parse(importEndTime) - time.parse(importStartTime)}'
          - importTimeoutUsageRatio: '${importDurationSeconds / importTimeoutSeconds}'
          - importTimedOut: '${importDurationSeconds >= importTimeoutSeconds}'
        next: emit_import_metrics_complete

    - emit_import_metrics_complete:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "complete"
            workflow: "genie-sim-import-pipeline"
            job: "genie-sim-import-job"
            scene_id: ${scene_id}
            job_id: ${job_id}
            timeout_seconds: ${importTimeoutSeconds}
            duration_seconds: ${importDurationSeconds}
            timeout_usage_ratio: ${importTimeoutUsageRatio}
            timed_out: ${importTimedOut}
            status: "SUCCEEDED"
            execution_name: ${importExecutionName}
            start_time: ${importStartTime}
            end_time: ${importEndTime}
          severity: "INFO"
        next: log_import_success_message

    - log_import_success_message:
        call: sys.log
        args:
          text: ${"Import job completed successfully for " + job_id}
          severity: INFO

    # Load and validate import manifest
    - set_manifest_location:
        assign:
          - manifest_object: ${"scenes/" + scene_id + "/episodes/geniesim_" + job_id + "/import_manifest.json"}
          - delivery_manifest_object: ${"scenes/" + scene_id + "/geniesim/import_manifest.json"}
          - delivery_manifest_path: ${"gs://" + bucket + "/" + "scenes/" + scene_id + "/geniesim/import_manifest.json"}
        next: read_import_manifest

    - read_import_manifest:
        try:
          call: googleapis.storage.v1.objects.get
          args:
            bucket: ${bucket}
            object: ${manifest_object}
            alt: "media"
          result: manifest_raw
        except:
          as: e
          steps:
            - log_manifest_missing:
                call: sys.log
                args:
                  text: ${"Import manifest missing at " + manifest_object + ": " + e.message}
                  severity: "ERROR"
            - raise_manifest_missing:
                raise:
                  message: ${"Import manifest missing at " + manifest_object}

    - parse_import_manifest:
        try:
          assign:
            - import_manifest: ${json.decode(manifest_raw)}
        except:
          as: e
          steps:
            - log_manifest_malformed:
                call: sys.log
                args:
                  text: ${"Import manifest malformed for job " + job_id + ": " + e.message}
                  severity: "ERROR"
            - raise_manifest_malformed:
                raise:
                  message: ${"Import manifest malformed for job " + job_id}

    - validate_import_manifest:
        switch:
          - condition: ${!("episodes" in import_manifest) or !("quality" in import_manifest)}
            raise:
              message: ${"Import manifest missing required sections for job " + job_id}
          - condition: ${!("downloaded" in import_manifest.episodes) or !("passed_validation" in import_manifest.episodes) or !("average_score" in import_manifest.quality)}
            raise:
              message: ${"Import manifest missing required fields for job " + job_id}

    # Extract import results
    - parse_import_output:
        assign:
          - episodes_imported: ${default(import_manifest.episodes.passed_validation, 0)}
          - output_path: ${default(import_manifest.gcs_output_path, import_manifest.output_dir)}
          - delivery_failed_marker: ${"scenes/" + scene_id + "/geniesim/.dataset_delivery_failed"}
        next: write_import_marker

    - write_import_marker:
        switch:
          - condition: ${scene_id == null or scene_id == "" or scene_id == "unknown"}
            next: check_training_enabled
        next: create_import_marker

    - create_import_marker:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${bucket}
          name: ${"scenes/" + scene_id + "/geniesim/.geniesim_import_complete"}
          uploadType: "media"
          body: ${"{\"scene_id\": \"" + scene_id + "\", \"job_id\": \"" + job_id + "\", \"status\": \"completed\", \"timestamp\": \"" + time.format(sys.now()) + "\"}"}
        result: import_marker
        next: maybe_write_import_idempotency_marker

    - maybe_write_import_idempotency_marker:
        switch:
          - condition: ${scene_id == null or scene_id == "" or scene_id == "unknown"}
            next: copy_import_manifest_for_delivery
        next: write_import_idempotency_marker

    - write_import_idempotency_marker:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${bucket}
          name: ${importIdempotencyObject}
          uploadType: "media"
          body: ${json.encode({
            "scene_id": scene_id,
            "job_id": job_id,
            "status": "completed",
            "timestamp": time.format(sys.now())
          })}
        result: importIdempotencyWriteResult
        next: copy_import_manifest_for_delivery

    - copy_import_manifest_for_delivery:
        call: googleapis.storage.v1.objects.copy
        args:
          sourceBucket: ${bucket}
          sourceObject: ${manifest_object}
          destinationBucket: ${bucket}
          destinationObject: ${delivery_manifest_object}
        result: delivery_manifest_copy
        next: trigger_delivery_eventarc

    - trigger_delivery_eventarc:
        try:
          call: googleapis.eventarc.v1.projects.locations.channels.events.trigger
          args:
            parent: ${"projects/" + project_id + "/locations/" + region + "/channels/dataset-delivery-events"}
            body:
              event:
                type: "blueprintpipeline.dataset.delivery.requested"
                source: "geniesim-import"
                data:
                  bucket: ${bucket}
                  scene_id: ${scene_id}
                  job_id: ${job_id}
                  output_path: ${output_path}
                  import_manifest_path: ${delivery_manifest_path}
          result: delivery_trigger_result
        except:
          as: e
          steps:
            - log_delivery_job_error:
                call: sys.log
                args:
                  text: ${"Failed to trigger dataset delivery: " + e.message}
                  severity: "ERROR"
            - write_delivery_failed_marker:
                call: googleapis.storage.v1.objects.insert
                args:
                  bucket: ${bucket}
                  name: ${delivery_failed_marker}
                  uploadType: "media"
                  body: ${json.encode({
                    "scene_id": scene_id,
                    "job_id": job_id,
                    "output_path": output_path,
                    "import_manifest_path": delivery_manifest_path,
                    "status": "failed",
                    "timestamp": time.format(sys.now()),
                    "error": {
                      "code": "dataset_delivery_trigger_failed",
                      "message": e.message,
                      "type": "workflow_failure"
                    },
                    "context": {
                      "workflow_execution_id": sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID")
                    }
                  })}
        next: check_training_enabled

    # Trigger downstream training pipeline (optional)
    - check_training_enabled:
        switch:
          - condition: ${default(event.trigger_training, false)}
            steps:
              - log_training_trigger:
                  call: sys.log
                  args:
                    text: "Triggering training pipeline for imported episodes"
                    severity: INFO

              - trigger_training:
                  call: googleapis.eventarc.v1.projects.locations.channels.events.trigger
                  args:
                    parent: ${"projects/" + project_id + "/locations/" + region + "/channels/training-events"}
                    body:
                      event:
                        type: "blueprintpipeline.episodes.imported"
                        source: "geniesim-import"
                        data:
                          job_id: ${job_id}
                          scene_id: ${scene_id}
                          output_path: ${output_path}
                          episode_count: ${episodes_imported}
        next: reset_circuit_breaker_success

    - reset_circuit_breaker_success:
        switch:
          - condition: ${circuitBreakerObject == null}
            next: return_success
        next: write_circuit_breaker_success

    - write_circuit_breaker_success:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${bucket}
          name: ${circuitBreakerObject}
          uploadType: "media"
          body: ${json.encode({
            "scene_id": scene_id,
            "failure_count": 0,
            "threshold": circuitBreakerThreshold,
            "status": "closed",
            "updated_at": time.format(sys.now()),
            "last_success_at": time.format(sys.now()),
            "last_failure_at": circuitBreakerLastFailureAt,
            "last_failure_reason": null
          })}
        next: log_circuit_breaker_success

    - log_circuit_breaker_success:
        call: sys.log
        args:
          text: ${"Circuit breaker reset for scene " + scene_id + " after successful import."}
          severity: "INFO"
        next: return_success

    # Return success
    - return_success:
        return:
          status: "success"
          job_id: ${job_id}
          scene_id: ${scene_id}
          episodes_imported: ${episodes_imported}
          output_path: ${output_path}
          import_execution_id: ${import_job_execution.name}
