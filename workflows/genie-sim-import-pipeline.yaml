# =============================================================================
# Genie Sim Import Pipeline
# =============================================================================
#
# Imports generated episodes from Genie Sim 3.0 back into BlueprintPipeline.
#
# Trigger Methods:
# 1. Manual trigger via Eventarc:
#    gcloud eventarc triggers create geniesim-import-trigger \
#      --destination-workflow=genie-sim-import-pipeline \
#      --event-filters="type=manual.geniesim.job.completed"
#
# 2. Scheduled polling (fallback):
#    Cloud Scheduler → Pub/Sub → Eventarc → Workflow
#
# Pipeline Flow:
#   Genie Sim Job Complete → Import Job → Episode Validation → Training Pipeline
#

main:
  params: [event]
  steps:
    - init:
        assign:
          - project_id: ${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}
          - region: ${default(sys.get_env("WORKFLOW_REGION"), "us-central1")}
          - bucket: ${sys.get_env("GCS_BUCKET", "blueprintpipeline-data")}

    - extract_event:
        switch:
          # Direct workflow payload (e.g., local runner or poller)
          - condition: ${"job_id" in event}
            assign:
              - job_id: ${event.job_id}
              - status: ${event.status}
              - scene_id: ${default(event.scene_id, "unknown")}
              - local_episodes_prefix: ${default(event.local_episodes_prefix, null)}
              - job_metadata_path: ${default(event.job_metadata_path, null)}

          # Eventarc event (wrapped)
          - condition: ${"data" in event and "job_id" in event.data}
            assign:
              - job_id: ${event.data.job_id}
              - status: ${event.data.status}
              - scene_id: ${default(event.data.scene_id, "unknown")}
              - local_episodes_prefix: ${default(event.data.local_episodes_prefix, null)}
              - job_metadata_path: ${default(event.data.job_metadata_path, null)}

          # Default/manual trigger
          - condition: true
            assign:
              - job_id: ${event.job_id}
              - status: ${default(event.status, "completed")}
              - scene_id: ${default(event.scene_id, "unknown")}
              - local_episodes_prefix: ${default(event.local_episodes_prefix, null)}
              - job_metadata_path: ${default(event.job_metadata_path, null)}
        next: resolve_job_id

    - resolve_job_id:
        switch:
          - condition: ${job_id == null or job_id == ""}
            next: fetch_job_metadata
        next: validate_inputs

    - fetch_job_metadata:
        switch:
          - condition: ${scene_id == null or scene_id == "" or scene_id == "unknown"}
            next: missing_job_id
        next: read_job_metadata

    - read_job_metadata:
        try:
          call: googleapis.storage.v1.objects.get
          args:
            bucket: ${bucket}
            object: '${"scenes/" + scene_id + "/geniesim/job.json"}'
            alt: "media"
          result: jobMetadataRaw
        except:
          as: e
          steps:
            - log_missing_job_metadata:
                call: sys.log
                args:
                  text: ${"Failed to load stored job metadata for scene " + scene_id + ": " + e.message}
                  severity: "ERROR"
            - raise_missing_job_metadata:
                raise: ${e}
        next: parse_job_metadata

    - parse_job_metadata:
        assign:
          - jobMetadata: ${json.decode(jobMetadataRaw)}
          - job_id: ${jobMetadata.job_id}
        next: validate_inputs

    - missing_job_id:
        raise:
          message: "job_id is required and could not be resolved from scene metadata"

    - validate_inputs:
        switch:
          - condition: ${job_id == null or job_id == ""}
            raise:
              message: "job_id is required"
        next: set_job_metadata_path

    - set_job_metadata_path:
        switch:
          - condition: ${job_metadata_path == null and scene_id != null and scene_id != "" and scene_id != "unknown"}
            assign:
              - job_metadata_path: '${"scenes/" + scene_id + "/geniesim/job.json"}'
        next: maybe_read_job_metadata_for_import

    - maybe_read_job_metadata_for_import:
        switch:
          - condition: ${job_metadata_path != null and local_episodes_prefix == null}
            next: read_job_metadata_for_import
        next: log_start

    - read_job_metadata_for_import:
        try:
          call: googleapis.storage.v1.objects.get
          args:
            bucket: ${bucket}
            object: ${job_metadata_path}
            alt: "media"
          result: importJobMetadataRaw
        except:
          as: e
          steps:
            - log_import_job_metadata_missing:
                call: sys.log
                args:
                  text: ${"Failed to load import job metadata from " + job_metadata_path + ": " + e.message}
                  severity: "WARNING"
            - set_empty_import_job_metadata:
                assign:
                  - importJobMetadataRaw: "{}"
        next: parse_import_job_metadata

    - parse_import_job_metadata:
        assign:
          - importJobMetadata: ${json.decode(importJobMetadataRaw)}
          - local_episodes_prefix: ${default(map.get(map.get(importJobMetadata, "artifacts", {}), "episodes_prefix"), default(map.get(map.get(importJobMetadata, "artifacts", {}), "episodes_path"), local_episodes_prefix))}
        next: init_quality_gate_preflight

    # Preflight quality gate check (downstream guardrail)
    - init_quality_gate_preflight:
        assign:
          - qualityGateReportObject: ${"scenes/" + scene_id + "/episode-generation-job/quality_gate_report.json"}
          - qualityGateReportPath: ${"gs://" + bucket + "/" + qualityGateReportObject}
          - qualityGateFailedMarker: ${"scenes/" + scene_id + "/geniesim/.failed"}
          - importOutputPrefix: ${default(local_episodes_prefix, "scenes/" + scene_id + "/episodes")}
          - importFailedMarker: ${default(local_episodes_prefix, "scenes/" + scene_id + "/episodes") + "/.failed"}
          - qualityCertPrefix: ${"scenes/" + scene_id + "/episodes/"}
          - qualityCertObject: null
          - qualityCertPageToken: null
        next: maybe_check_quality_gates

    - maybe_check_quality_gates:
        switch:
          - condition: ${scene_id == null or scene_id == "" or scene_id == "unknown"}
            next: log_quality_gate_skip
        next: read_quality_gate_report

    - log_quality_gate_skip:
        call: sys.log
        args:
          text: "Skipping quality gate preflight; scene_id is missing."
          severity: "WARNING"
        next: log_start

    - read_quality_gate_report:
        try:
          call: googleapis.storage.v1.objects.get
          args:
            bucket: ${bucket}
            object: ${qualityGateReportObject}
            alt: "media"
          result: qualityGateReportRaw
        except:
          as: e
          steps:
            - handle_quality_gate_report_missing:
                switch:
                  - condition: ${e.code == 404}
                    next: list_quality_certificates
                next: raise_quality_gate_report_error

    - raise_quality_gate_report_error:
        raise:
          message: ${"Failed to read quality gate report at " + qualityGateReportPath}

    - parse_quality_gate_report:
        assign:
          - qualityGateReport: ${json.decode(qualityGateReportRaw)}
        next: evaluate_quality_gate_report

    - evaluate_quality_gate_report:
        switch:
          - condition: ${"summary" in qualityGateReport and qualityGateReport.summary.can_proceed == true and qualityGateReport.summary.blocking_failures == 0}
            next: log_quality_gate_passed
        next: quality_gate_failed

    - list_quality_certificates:
        call: googleapis.storage.v1.objects.list
        args:
          bucket: ${bucket}
          prefix: ${qualityCertPrefix}
          maxResults: 1000
          pageToken: ${qualityCertPageToken}
        result: qualityCertList
        next: scan_quality_certificates

    - scan_quality_certificates:
        for:
          value: certItem
          in: ${default(qualityCertList.items, [])}
          steps:
            - check_quality_cert_match:
                switch:
                  - condition: ${qualityCertObject == null and text.match_regex(certItem.name, "quality_certificate\\.json$")}
                    assign:
                      - qualityCertObject: ${certItem.name}
        next: check_quality_cert_scan_results

    - check_quality_cert_scan_results:
        switch:
          - condition: ${qualityCertObject != null}
            next: read_quality_certificate
          - condition: ${qualityCertList.nextPageToken != null}
            assign:
              - qualityCertPageToken: ${qualityCertList.nextPageToken}
            next: list_quality_certificates
        next: quality_gate_missing_artifacts

    - read_quality_certificate:
        call: googleapis.storage.v1.objects.get
        args:
          bucket: ${bucket}
          object: ${qualityCertObject}
          alt: "media"
        result: qualityCertRaw
        next: parse_quality_certificate

    - parse_quality_certificate:
        assign:
          - qualityCertificate: ${json.decode(qualityCertRaw)}
        next: evaluate_quality_certificate

    - evaluate_quality_certificate:
        switch:
          - condition: ${qualityCertificate.validation_passed == true and qualityCertificate.recommended_use != "testing"}
            next: log_quality_gate_passed_with_certificate
        next: quality_gate_failed

    - log_quality_gate_passed_with_certificate:
        call: sys.log
        args:
          text: ${"Quality gate report missing; proceeding based on certificate " + qualityCertObject}
          severity: "WARNING"
        next: log_start

    - quality_gate_missing_artifacts:
        call: sys.log
        args:
          text: ${"Missing quality gate report and quality certificates for scene " + scene_id}
          severity: "ERROR"
        next: quality_gate_failed

    - log_quality_gate_passed:
        call: sys.log
        args:
          text: ${"Quality gate passed for scene " + scene_id}
          severity: "INFO"
        next: log_start

    - quality_gate_failed:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${bucket}
          name: ${qualityGateFailedMarker}
          uploadType: "media"
          body: ${json.encode({
            "scene_id": scene_id,
            "status": "failed",
            "timestamp": time.format(sys.now()),
            "error": {
              "code": "quality_gate_failed",
              "message": "Downstream quality gate preflight blocked import",
              "report_path": qualityGateReportPath
            }
          })}
        result: qualityGateFailedMarkerResult
        next: raise_quality_gate_failed

    - raise_quality_gate_failed:
        raise:
          message: ${"Quality gate failed; report: " + qualityGateReportPath}

    - log_start:
        call: sys.log
        args:
          text: ${"Starting Genie Sim import for job " + job_id + " (scene: " + scene_id + ")"}
          severity: INFO

    # Check if job completed successfully
    - check_status:
        switch:
          - condition: ${status != "completed"}
            steps:
              - log_skip:
                  call: sys.log
                  args:
                    text: ${"Job " + job_id + " status is '" + status + "', skipping import"}
                    severity: WARNING
              - return_skipped:
                  return:
                    status: "skipped"
                    job_id: ${job_id}
                    reason: ${"Job status is " + status}

    # Run import job
    - init_import_metrics:
        assign:
          - importStartTime: '${time.format(sys.now())}'
          - importTimeoutSeconds: 1800
        next: emit_import_metrics_start

    - emit_import_metrics_start:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "start"
            workflow: "genie-sim-import-pipeline"
            job: "genie-sim-import-job"
            scene_id: ${scene_id}
            job_id: ${job_id}
            timeout_seconds: ${importTimeoutSeconds}
            retry_max: 5
            start_time: ${importStartTime}
          severity: "INFO"
        next: run_import_job

    - run_import_job:
        try:
          call: googleapis.run.v2.projects.locations.jobs.run
          args:
            name: ${"projects/" + project_id + "/locations/" + region + "/jobs/genie-sim-import-job"}
            body:
              overrides:
                containerOverrides:
                  - name: "genie-sim-import"
                    env:
                      # Supported overrides via workflow payload:
                      #   min_quality_score, enable_validation, filter_low_quality, wait_for_completion
                      - name: BUCKET
                        value: ${bucket}
                      - name: GENIE_SIM_JOB_ID
                        value: ${job_id}
                      - name: SCENE_ID
                        value: ${scene_id}
                      - name: WAIT_FOR_COMPLETION
                        value: ${default(event.wait_for_completion, "true")}
                      - name: MIN_QUALITY_SCORE
                        value: ${default(event.min_quality_score, "0.85")}
                      - name: ENABLE_VALIDATION
                        value: ${default(event.enable_validation, "true")}
                      - name: FILTER_LOW_QUALITY
                        value: ${default(event.filter_low_quality, "true")}
                      - name: JOB_METADATA_PATH
                        value: ${default(job_metadata_path, "")}
                      - name: LOCAL_EPISODES_PREFIX
                        value: ${default(local_episodes_prefix, "")}
                timeout: "1800s"  # 30 minutes max
          result: import_job_execution
        retry:
          predicate: ${http.default_retry_predicate}
          max_retries: 5
          backoff:
            initial_delay: 1
            max_delay: 60
            multiplier: 2
        except:
          as: e
          steps:
            - log_import_retry_exhausted:
                call: sys.log
                args:
                  data:
                    bp_metric: "job_retry_exhausted"
                    workflow: "genie-sim-import-pipeline"
                    job: "genie-sim-import-job"
                    scene_id: ${scene_id}
                    job_id: ${job_id}
                    retry_max: 5
                    error: ${e.message}
                  severity: "ERROR"
            - log_import_job_error:
                call: sys.log
                args:
                  text: ${"Failed to start genie-sim-import job after retries: " + e.message}
                  severity: "ERROR"
            - list_import_outputs_for_cleanup:
                call: googleapis.storage.v1.objects.list
                args:
                  bucket: ${bucket}
                  prefix: ${importOutputPrefix + "/"}
                  maxResults: 1000
                result: importCleanupList
            - delete_import_outputs_for_cleanup:
                for:
                  value: importCleanupItem
                  in: ${default(importCleanupList.items, [])}
                  steps:
                    - delete_import_output:
                        call: googleapis.storage.v1.objects.delete
                        args:
                          bucket: ${bucket}
                          object: ${importCleanupItem.name}
            - write_import_failure_marker:
                call: googleapis.storage.v1.objects.insert
                args:
                  bucket: ${bucket}
                  name: ${importFailedMarker}
                  uploadType: "media"
                  body: ${json.encode({
                    "scene_id": scene_id,
                    "job_name": "genie-sim-import-job",
                    "status": "failed",
                    "timestamp": time.format(sys.now()),
                    "error": {
                      "code": "genie_sim_import_start_failed",
                      "message": e.message,
                      "type": "workflow_failure",
                      "stack_trace": null
                    },
                    "context": {
                      "workflow_execution_id": sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),
                      "attempt_number": 1,
                      "config_context": {
                        "output_prefix": importOutputPrefix
                      }
                    }
                  })}
            - raise_import_job_error:
                raise: ${e}

    - set_import_execution_name:
        assign:
          - importExecutionName: '${if(import_job_execution.metadata != null and import_job_execution.metadata.name != null, import_job_execution.metadata.name, import_job_execution.name)}'
        next: wait_for_import

    - wait_for_import:
        call: googleapis.run.v2.projects.locations.jobs.executions.get
        args:
          name: ${importExecutionName}
        result: import_status
        next: check_import_status

    - check_import_status:
        assign:
          - importState: '${if(import_status.state != null, import_status.state, if(import_status.status != null, import_status.status.state, null))}'
          - importFailedCount: '${if(import_status.failedCount != null, import_status.failedCount, if(import_status.status != null, import_status.status.failedCount, null))}'
          - importSucceededCount: '${if(import_status.succeededCount != null, import_status.succeededCount, if(import_status.status != null, import_status.status.succeededCount, null))}'
        next: import_status_switch

    - import_status_switch:
        switch:
          - condition: '${importState == "FAILED" or (importFailedCount != null and importFailedCount > 0)}'
            next: check_import_success
          - condition: '${importState == "SUCCEEDED" or (importSucceededCount != null and importSucceededCount > 0)}'
            next: log_import_success
        next: wait_import_poll

    - wait_import_poll:
        call: sys.sleep
        args:
          seconds: 10
        next: wait_for_import

    - check_import_success:
        assign:
          - importEndTime: '${time.format(sys.now())}'
          - importDurationSeconds: '${time.parse(importEndTime) - time.parse(importStartTime)}'
          - importTimeoutUsageRatio: '${importDurationSeconds / importTimeoutSeconds}'
          - importTimedOut: '${importDurationSeconds >= importTimeoutSeconds}'
        next: emit_import_metrics_failed

    - emit_import_metrics_failed:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "complete"
            workflow: "genie-sim-import-pipeline"
            job: "genie-sim-import-job"
            scene_id: ${scene_id}
            job_id: ${job_id}
            timeout_seconds: ${importTimeoutSeconds}
            duration_seconds: ${importDurationSeconds}
            timeout_usage_ratio: ${importTimeoutUsageRatio}
            timed_out: ${importTimedOut}
            status: "FAILED"
            execution_name: ${importExecutionName}
            start_time: ${importStartTime}
            end_time: ${importEndTime}
          severity: "ERROR"
        next: log_import_failed_message

    - log_import_failed_message:
        call: sys.log
        args:
          text: ${"Import job failed for job " + job_id}
          severity: ERROR
        next: raise_import_error

    - raise_import_error:
        raise:
          message: ${"Import failed for job " + job_id}

    - log_import_success:
        assign:
          - importEndTime: '${time.format(sys.now())}'
          - importDurationSeconds: '${time.parse(importEndTime) - time.parse(importStartTime)}'
          - importTimeoutUsageRatio: '${importDurationSeconds / importTimeoutSeconds}'
          - importTimedOut: '${importDurationSeconds >= importTimeoutSeconds}'
        next: emit_import_metrics_complete

    - emit_import_metrics_complete:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "complete"
            workflow: "genie-sim-import-pipeline"
            job: "genie-sim-import-job"
            scene_id: ${scene_id}
            job_id: ${job_id}
            timeout_seconds: ${importTimeoutSeconds}
            duration_seconds: ${importDurationSeconds}
            timeout_usage_ratio: ${importTimeoutUsageRatio}
            timed_out: ${importTimedOut}
            status: "SUCCEEDED"
            execution_name: ${importExecutionName}
            start_time: ${importStartTime}
            end_time: ${importEndTime}
          severity: "INFO"
        next: log_import_success_message

    - log_import_success_message:
        call: sys.log
        args:
          text: ${"Import job completed successfully for " + job_id}
          severity: INFO

    # Load and validate import manifest
    - set_manifest_location:
        assign:
          - manifest_object: ${"scenes/" + scene_id + "/episodes/geniesim_" + job_id + "/import_manifest.json"}
          - delivery_manifest_object: ${"scenes/" + scene_id + "/geniesim/import_manifest.json"}
        next: read_import_manifest

    - read_import_manifest:
        try:
          call: googleapis.storage.v1.objects.get
          args:
            bucket: ${bucket}
            object: ${manifest_object}
            alt: "media"
          result: manifest_raw
        except:
          as: e
          steps:
            - log_manifest_missing:
                call: sys.log
                args:
                  text: ${"Import manifest missing at " + manifest_object + ": " + e.message}
                  severity: "ERROR"
            - raise_manifest_missing:
                raise:
                  message: ${"Import manifest missing at " + manifest_object}

    - parse_import_manifest:
        try:
          assign:
            - import_manifest: ${json.decode(manifest_raw)}
        except:
          as: e
          steps:
            - log_manifest_malformed:
                call: sys.log
                args:
                  text: ${"Import manifest malformed for job " + job_id + ": " + e.message}
                  severity: "ERROR"
            - raise_manifest_malformed:
                raise:
                  message: ${"Import manifest malformed for job " + job_id}

    - validate_import_manifest:
        switch:
          - condition: ${!("episodes" in import_manifest) or !("quality" in import_manifest)}
            raise:
              message: ${"Import manifest missing required sections for job " + job_id}
          - condition: ${!("downloaded" in import_manifest.episodes) or !("passed_validation" in import_manifest.episodes) or !("average_score" in import_manifest.quality)}
            raise:
              message: ${"Import manifest missing required fields for job " + job_id}

    # Extract import results
    - parse_import_output:
        assign:
          - episodes_imported: ${default(import_manifest.episodes.passed_validation, 0)}
          - output_path: ${default(import_manifest.gcs_output_path, import_manifest.output_dir)}
        next: write_import_marker

    - write_import_marker:
        switch:
          - condition: ${scene_id == null or scene_id == "" or scene_id == "unknown"}
            next: check_training_enabled
        next: create_import_marker

    - create_import_marker:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${bucket}
          name: ${"scenes/" + scene_id + "/geniesim/.geniesim_import_complete"}
          uploadType: "media"
          body: ${"{\"scene_id\": \"" + scene_id + "\", \"job_id\": \"" + job_id + "\", \"status\": \"completed\", \"timestamp\": \"" + time.format(sys.now()) + "\"}"}
        result: import_marker
        next: copy_import_manifest_for_delivery

    - copy_import_manifest_for_delivery:
        call: googleapis.storage.v1.objects.copy
        args:
          sourceBucket: ${bucket}
          sourceObject: ${manifest_object}
          destinationBucket: ${bucket}
          destinationObject: ${delivery_manifest_object}
        result: delivery_manifest_copy
        next: run_delivery_job

    - run_delivery_job:
        try:
          call: googleapis.run.v2.projects.locations.jobs.run
          args:
            name: ${"projects/" + project_id + "/locations/" + region + "/jobs/dataset-delivery-job"}
            body:
              overrides:
                containerOverrides:
                  - env:
                      - name: BUCKET
                        value: ${bucket}
                      - name: SCENE_ID
                        value: ${scene_id}
                      - name: JOB_ID
                        value: ${job_id}
                      - name: IMPORT_MANIFEST_PATH
                        value: ${"gs://" + bucket + "/" + delivery_manifest_object}
                timeout: "1800s"
          result: delivery_job_execution
        except:
          as: e
          steps:
            - log_delivery_job_error:
                call: sys.log
                args:
                  text: ${"Failed to start dataset delivery job: " + e.message}
                  severity: "ERROR"
        next: check_training_enabled

    # Trigger downstream training pipeline (optional)
    - check_training_enabled:
        switch:
          - condition: ${default(event.trigger_training, false)}
            steps:
              - log_training_trigger:
                  call: sys.log
                  args:
                    text: "Triggering training pipeline for imported episodes"
                    severity: INFO

              - trigger_training:
                  call: googleapis.eventarc.v1.projects.locations.channels.events.trigger
                  args:
                    parent: ${"projects/" + project_id + "/locations/" + region + "/channels/training-events"}
                    body:
                      event:
                        type: "blueprintpipeline.episodes.imported"
                        source: "geniesim-import"
                        data:
                          job_id: ${job_id}
                          scene_id: ${scene_id}
                          output_path: ${output_path}
                          episode_count: ${episodes_imported}

    # Return success
    - return_success:
        return:
          status: "success"
          job_id: ${job_id}
          scene_id: ${scene_id}
          episodes_imported: ${episodes_imported}
          output_path: ${output_path}
          import_execution_id: ${import_job_execution.name}
