# episode-generation-pipeline.yaml
#
# Google Cloud Workflows pipeline that:
#   1. Triggers on completion marker file from usd-assembly-job (.usd_complete)
#   2. Runs episode-generation-job on GKE with GPU (Isaac Sim)
#   3. Generates real physics-validated training episodes
#   4. Writes .episodes_complete marker when finished
#
# NOTE: Unlike other pipeline jobs, episode generation requires GPU (Isaac Sim)
# so it runs on GKE instead of Cloud Run.
#
# Trigger: Cloud Storage object finalized event for .usd_complete
#
# EventArc Setup:
#   gcloud eventarc triggers create episode-generation-trigger \
#     --location=us-central1 \
#     --service-account="${WORKFLOW_SA}@${PROJECT_ID}.iam.gserviceaccount.com" \
#     --destination-workflow=episode-generation-pipeline \
#     --destination-workflow-location=us-central1 \
#     --event-filters="type=google.cloud.storage.object.v1.finalized" \
#     --event-filters="bucket=${BUCKET}" \
#     --event-data-content-type="application/json"
#
# Or run manually:
#   gcloud workflows run episode-generation-pipeline \
#     --location=us-central1 \
#     --data='{"data":{"bucket":"your-bucket","name":"scenes/kitchen_001/usd/.usd_complete"}}'
#
# Configurable inputs/environment:
#   - build_poll_timeout_seconds: Max seconds to poll Cloud Build for episode job completion.
#     Defaults to EPISODE_BUILD_POLL_TIMEOUT_SECONDS env var or 3600 seconds.

main:
  params: [event]
  steps:
    - log_event:
        call: sys.log
        args:
          text: '${event}'
          severity: "INFO"
        next: extract

    - extract:
        assign:
          - bucket: ${event.data.bucket}
          - object: ${event.data.name}
          - projectId: '${sys.get_env("GOOGLE_CLOUD_PROJECT_ID")}'
          - primaryRegion: ${default(sys.get_env("PRIMARY_WORKFLOW_REGION"), "us-central1")}
          - secondaryRegion: ${default(sys.get_env("SECONDARY_WORKFLOW_REGION"), "us-east1")}
          - region: ${primaryRegion}
          - primaryGkeCluster: '${default(sys.get_env("PRIMARY_GKE_CLUSTER"), "blueprint-cluster")}'
          - secondaryGkeCluster: '${default(sys.get_env("SECONDARY_GKE_CLUSTER"), "blueprint-cluster-secondary")}'
          - primaryGkeZone: '${default(sys.get_env("PRIMARY_GKE_ZONE"), "us-central1-a")}'
          - secondaryGkeZone: '${default(sys.get_env("SECONDARY_GKE_ZONE"), "us-east1-b")}'
          - gkeCluster: '${primaryGkeCluster}'
          - gkeZone: '${primaryGkeZone}'
          - namespace: "blueprint"
          - isProduction: '${sys.get_env("PIPELINE_ENV") == "production"}'
          - enforceQualityGates: '${default(map.get(event, "enforce_quality_gates"), sys.get_env("PIPELINE_ENV") == "production")}'
          - runStagingE2E: '${default(map.get(event, "run_staging_e2e"), sys.get_env("PIPELINE_ENV") == "production")}'
          - stagingDataRoot: '${default(map.get(event, "staging_data_root"), "/mnt/gcs")}'
          - monitoringGateEnabled: '${default(map.get(event, "monitoring_gate_enabled"), sys.get_env("PIPELINE_ENV") == "production")}'
          # Production runs always enforce strict monitoring gates (no bypass).
          - monitoringGateStrict: '${if(isProduction, true, default(map.get(event, "monitoring_gate_strict"), default(sys.get_env("MONITORING_GATE_STRICT"), "true")) == "true")}'
          - buildPollTimeoutSeconds: '${int(default(map.get(event, "build_poll_timeout_seconds"), default(sys.get_env("EPISODE_BUILD_POLL_TIMEOUT_SECONDS"), "3600")))}'
          - firebaseBucket: '${default(sys.get_env("FIREBASE_STORAGE_BUCKET"), "")}'
          - firebaseServiceAccountJson: '${default(sys.get_env("FIREBASE_SERVICE_ACCOUNT_JSON"), "")}'
          - firebaseServiceAccountPath: '${default(sys.get_env("FIREBASE_SERVICE_ACCOUNT_PATH"), "/secrets/firebase/service-account.json")}'
          - firebaseUploadPrefix: '${default(sys.get_env("FIREBASE_UPLOAD_PREFIX"), "datasets")}'
          - enableFirebaseUpload: '${default(sys.get_env("ENABLE_FIREBASE_UPLOAD"), if(sys.get_env("PIPELINE_ENV") == "production", "true", "false"))}'
          # Bundle tier for upsell features (standard, pro, enterprise, foundation)
          # Can be passed in event or read from scene config
          - bundleTier: '${default(map.get(event, "bundle_tier"), "standard")}'
          # Data pack tier derived from bundle tier:
          # standard -> core, pro -> plus, enterprise/foundation -> full
          - dataPackTier: '${if(bundleTier == "standard", "core", if(bundleTier == "pro", "plus", "full"))}'
          # Episodes per variation scales with bundle tier
          - episodesPerVariation: '${if(bundleTier == "foundation", "25", "10")}'
          # Audio/subtitle defaults (can be overridden by scene config)
          - audioNarrationEnabled: false
          - subtitleGenerationEnabled: false
        next: filter_completion_markers

    # Filter for .usd_complete marker files (from USD assembly job)
    - filter_completion_markers:
        switch:
          - condition: '${text.match_regex(object, "^scenes/.+/usd/\\.usd_complete$")}'
            next: derive
        next: skip

    - derive:
        assign:
          - parts: '${text.split(object, "/")}'
          - sceneId: ${parts[1]}
          - assetsPrefix: '${"scenes/" + sceneId + "/assets"}'
          - usdPrefix: '${"scenes/" + sceneId + "/usd"}'
          - episodesPrefix: '${"scenes/" + sceneId + "/episodes"}'
          - episodeJobName: "episode-generation-job"
          - episodeJobTimeoutSeconds: 21600
          - episodesCompleteMarker: '${"scenes/" + sceneId + "/episodes/.episodes_complete"}'
          - episodesFailedMarker: '${"scenes/" + sceneId + "/episodes/.failed"}'
          - episodesLegacyFailedMarker: '${"scenes/" + sceneId + "/episodes/.episodes_failed"}'
          - sceneConfigPath: '${"scenes/" + sceneId + "/config.json"}'
          - qualityGateJobName: "quality-gate-job"
          - qualityGateTimeoutSeconds: 1800
        next: init_region_health

    - init_region_health:
        assign:
          - primaryHealthy: false
          - secondaryHealthy: false
        next: check_primary_cluster

    - check_primary_cluster:
        try:
          call: googleapis.container.v1.projects.locations.clusters.get
          args:
            name: '${"projects/" + projectId + "/locations/" + primaryGkeZone + "/clusters/" + primaryGkeCluster}'
          result: primaryCluster
        next: mark_primary_healthy
        except:
          as: primaryError
          steps:
            - log_primary_unhealthy:
                call: sys.log
                args:
                  text: '${"Primary GKE cluster " + primaryGkeCluster + " unavailable: " + primaryError.message}'
                  severity: "WARNING"
            - check_secondary_cluster:
                try:
                  call: googleapis.container.v1.projects.locations.clusters.get
                  args:
                    name: '${"projects/" + projectId + "/locations/" + secondaryGkeZone + "/clusters/" + secondaryGkeCluster}'
                  result: secondaryCluster
                next: mark_secondary_healthy
                except:
                  as: secondaryError
                  steps:
                    - log_secondary_unhealthy:
                        call: sys.log
                        args:
                          text: '${"Secondary GKE cluster " + secondaryGkeCluster + " unavailable: " + secondaryError.message}'
                          severity: "ERROR"
                    - select_cluster

    - mark_primary_healthy:
        assign:
          - primaryHealthy: true
        next: select_cluster

    - mark_secondary_healthy:
        assign:
          - secondaryHealthy: true
        next: select_cluster

    - select_cluster:
        switch:
          - condition: ${primaryHealthy}
            next: use_primary_cluster
          - condition: ${secondaryHealthy}
            next: use_secondary_cluster
        next: use_primary_cluster

    - use_primary_cluster:
        assign:
          - gkeCluster: ${primaryGkeCluster}
          - gkeZone: ${primaryGkeZone}
          - region: ${primaryRegion}
        next: lookup_scene_config

    - use_secondary_cluster:
        assign:
          - gkeCluster: ${secondaryGkeCluster}
          - gkeZone: ${secondaryGkeZone}
          - region: ${secondaryRegion}
        next: lookup_scene_config

    # Lookup scene config from GCS to get customer-specific bundle tier
    - lookup_scene_config:
        try:
          call: googleapis.storage.v1.objects.get
          args:
            bucket: ${bucket}
            object: ${sceneConfigPath}
            alt: "media"
          result: sceneConfigRaw
        except:
          as: e
          steps:
            - log_no_config:
                call: sys.log
                args:
                  text: '${"No scene config found for " + sceneId + ", using event tier or defaults"}'
                  severity: "INFO"
                next: check_already_processed
        next: parse_scene_config

    # Parse scene config and override bundle tier if specified
    - parse_scene_config:
        try:
          assign:
            # If scene config has bundle_tier, use it (customer-specific)
            - sceneConfig: ${json.decode(sceneConfigRaw)}
            - configBundleTier: '${default(map.get(sceneConfig, "bundle_tier"), "")}'
            # Override bundleTier if config specifies one
            - bundleTier: '${if(configBundleTier != "", configBundleTier, bundleTier)}'
            # Re-derive dependent variables after bundle tier update
            - dataPackTier: '${if(bundleTier == "standard", "core", if(bundleTier == "pro", "plus", "full"))}'
            - episodesPerVariation: '${if(bundleTier == "foundation", "25", "10")}'
            # Get audio/subtitle settings from config
            - audioNarrationEnabled: '${default(map.get(sceneConfig, "audio_narration_enabled"), false)}'
            - subtitleGenerationEnabled: '${default(map.get(sceneConfig, "subtitle_generation_enabled"), false)}'
        except:
          as: e
          steps:
            - log_parse_error:
                call: sys.log
                args:
                  text: '${"Error parsing scene config: " + e.message}'
                  severity: "WARNING"
        next: log_config_resolved

    - log_config_resolved:
        call: sys.log
        args:
          text: '${"Scene " + sceneId + " resolved config - tier: " + bundleTier + ", data_pack: " + dataPackTier}'
          severity: "INFO"
        next: check_monitoring_gate

    - check_monitoring_gate:
        switch:
          - condition: '${monitoringGateEnabled == true}'
            next: list_monitoring_dashboards
        next: check_already_processed

    - list_monitoring_dashboards:
        call: googleapis.monitoring.v1.projects.dashboards.list
        args:
          parent: '${"projects/" + projectId}'
        result: dashboardsResult
        next: list_monitoring_alerts

    - list_monitoring_alerts:
        call: googleapis.monitoring.v3.projects.alertPolicies.list
        args:
          parent: '${"projects/" + projectId}'
        result: alertPoliciesResult
        next: list_monitoring_log_metrics

    - list_monitoring_log_metrics:
        call: googleapis.logging.v2.projects.metrics.list
        args:
          parent: '${"projects/" + projectId}'
        result: logMetricsResult
        next: evaluate_monitoring_gate

    - evaluate_monitoring_gate:
        assign:
          - dashboardsPayload: '${json.encode(dashboardsResult)}'
          - alertsPayload: '${json.encode(alertPoliciesResult)}'
          - metricsPayload: '${json.encode(logMetricsResult)}'
          - dashboardOverviewOk: '${text.contains(dashboardsPayload, "BlueprintPipeline - Overview")}'
          - dashboardGpuOk: '${text.contains(dashboardsPayload, "BlueprintPipeline - GPU Metrics")}'
          - alertTimeoutOk: '${text.contains(alertsPayload, "[Blueprint] Workflow Job Timeout Detected")}'
          - alertRetryOk: '${text.contains(alertsPayload, "[Blueprint] Workflow Job Retry Spike")}'
          - metricTimeoutEventsOk: '${text.contains(metricsPayload, "blueprint_job_timeout_events")}'
          - metricRetryExhaustedOk: '${text.contains(metricsPayload, "blueprint_job_retry_exhausted_total")}'
          - metricTimeoutUsageOk: '${text.contains(metricsPayload, "blueprint_job_timeout_usage_ratio")}'
          - metricSlaViolationsOk: '${text.contains(metricsPayload, "blueprint_geniesim_sla_violations")}'
          - metricFailureEventsOk: '${text.contains(metricsPayload, "blueprint_job_failure_events")}'
          - dashboardsOk: '${dashboardOverviewOk && dashboardGpuOk}'
          - alertsOk: '${alertTimeoutOk && alertRetryOk}'
          - metricsOk: '${metricTimeoutEventsOk && metricRetryExhaustedOk && metricTimeoutUsageOk && metricSlaViolationsOk && metricFailureEventsOk}'
          - monitoringGateOk: '${dashboardsOk && alertsOk && metricsOk}'
        next: monitoring_gate_decision

    - monitoring_gate_decision:
        switch:
          - condition: '${monitoringGateOk == true}'
            next: check_already_processed
        next: monitoring_gate_failed

    - monitoring_gate_failed:
        call: sys.log
        args:
          text: '${"Monitoring gate failed. Missing assets (false indicates missing): dashboard_overview_ok=" + json.encode(dashboardOverviewOk) + ", dashboard_gpu_ok=" + json.encode(dashboardGpuOk) + ", alert_timeout_ok=" + json.encode(alertTimeoutOk) + ", alert_retry_ok=" + json.encode(alertRetryOk) + ", metric_timeout_events_ok=" + json.encode(metricTimeoutEventsOk) + ", metric_retry_exhausted_ok=" + json.encode(metricRetryExhaustedOk) + ", metric_timeout_usage_ok=" + json.encode(metricTimeoutUsageOk) + ", metric_sla_violations_ok=" + json.encode(metricSlaViolationsOk) + ", metric_failure_events_ok=" + json.encode(metricFailureEventsOk)}'
          severity: '${if(monitoringGateStrict, "ERROR", "WARNING")}'
        next: monitoring_gate_strict_decision

    - monitoring_gate_strict_decision:
        switch:
          # Production mode must never bypass monitoring gate failures.
          - condition: '${isProduction == true}'
            next: raise_monitoring_gate_error
          - condition: '${monitoringGateStrict == true}'
            next: raise_monitoring_gate_error
        next: monitoring_gate_bypass

    - monitoring_gate_bypass:
        call: sys.log
        args:
          text: '${"Monitoring gate strict mode disabled; continuing despite missing monitoring assets."}'
          severity: "WARNING"
        next: check_already_processed

    - raise_monitoring_gate_error:
        raise: '${"Monitoring gate failed; deploy dashboards, alert policies, and log-based metrics before production runs."}'

    # Check if episodes already generated (idempotence)
    - check_already_processed:
        try:
          call: googleapis.storage.v1.objects.get
          args:
            bucket: ${bucket}
            object: ${episodesCompleteMarker}
          result: existingMarker
        except:
          as: e
          steps:
            - check_not_found:
                switch:
                  - condition: '${e.code == 404}'
                    next: log_start
                next: raise_check_error
        next: skip_already_processed

    - skip_already_processed:
        call: sys.log
        args:
          text: '${"Episodes already generated for scene " + sceneId + " - skipping"}'
          severity: "INFO"
        next: skip

    - raise_check_error:
        raise: '${e}'

    - log_start:
        call: sys.log
        args:
          text: '${"Episode generation triggered for scene " + sceneId + " (Isaac Sim GPU job)"}'
          severity: "INFO"
        next: init_episode_metrics

    - init_episode_metrics:
        assign:
          - episodeStartTime: '${time.format(sys.now())}'
        next: emit_episode_metrics_start

    - emit_episode_metrics_start:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "start"
            workflow: "episode-generation-pipeline"
            job: ${episodeJobName}
            scene_id: ${sceneId}
            timeout_seconds: ${episodeJobTimeoutSeconds}
            duration_seconds: 0
            timeout_usage_ratio: 0
            timed_out: false
            status: "STARTED"
            start_time: ${episodeStartTime}
          severity: "INFO"
        next: create_gke_job

    # Create a Kubernetes Job on GKE for episode generation
    # This uses the GKE API to create a Job with GPU resources
    - create_gke_job:
        call: http.post
        args:
          url: '${"https://container.googleapis.com/v1/projects/" + projectId + "/zones/" + gkeZone + "/clusters/" + gkeCluster + ":setMasterAuth"}'
          auth:
            type: OAuth2
          # For GKE job creation, we use kubectl via Cloud Build or a sidecar
          # Here we'll use a simpler approach: trigger a Cloud Build that creates the job
        result: gkeAuth
        next: run_episode_job_via_cloudbuild

    # Alternative: Run episode generation via Cloud Build (which can access GKE)
    - run_episode_job_via_cloudbuild:
        try:
          call: googleapis.cloudbuild.v1.projects.builds.create
          args:
            projectId: ${projectId}
            body:
              steps:
                - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
                  entrypoint: 'bash'
                  args:
                    - '-c'
                    - |
                      set -e

                      # Get GKE credentials
                      gcloud container clusters get-credentials ${gkeCluster} \
                        --zone=${gkeZone} \
                        --project=${projectId}

                      # Preflight guard: ensure GPU nodes are present before scheduling
                      GPU_NODE_COUNT=$(kubectl get nodes \
                        -l cloud.google.com/gke-accelerator=nvidia-tesla-t4 \
                        -o name | wc -l | tr -d ' ')
                      if [ "${GPU_NODE_COUNT}" -eq 0 ]; then
                        echo "ERROR: No GPU nodes found with accelerator label nvidia-tesla-t4."
                        echo "Episode generation requires Isaac Sim runtime on GPU nodes."
                        exit 1
                      fi

                      GPU_CAPACITY=$(kubectl get nodes \
                        -l cloud.google.com/gke-accelerator=nvidia-tesla-t4 \
                        -o jsonpath='{range .items[*]}{.status.capacity.nvidia\.com/gpu}{"\n"}{end}' \
                        | awk 'BEGIN{sum=0} {if ($1 ~ /^[0-9]+$/) sum+=$1} END{print sum}')
                      if [ "${GPU_CAPACITY:-0}" -le 0 ]; then
                        echo "ERROR: GPU capacity is not reported on GPU nodes."
                        echo "Verify NVIDIA drivers and device plugin are installed."
                        exit 1
                      fi

                      # Create unique job name
                      JOB_NAME="episode-gen-${sceneId}-$(date +%s)"

                      # Create the Kubernetes Job
                      cat <<EOFK8S | kubectl apply -f -
                      apiVersion: batch/v1
                      kind: Job
                      metadata:
                        name: ${JOB_NAME}
                        namespace: ${namespace}
                        labels:
                          app: episode-generation
                          scene-id: ${sceneId}
                          triggered-by: workflow
                      spec:
                        backoffLimit: 2
                        activeDeadlineSeconds: 21600  # 6 hours max
                        template:
                          metadata:
                            labels:
                              app: episode-generation
                          spec:
                            restartPolicy: Never
                            serviceAccountName: blueprint-pipeline-sa
                            nodeSelector:
                              cloud.google.com/gke-accelerator: nvidia-tesla-t4
                            tolerations:
                            - key: nvidia.com/gpu
                              operator: Equal
                              value: present
                              effect: NoSchedule
                            containers:
                            - name: episode-generation
                              image: ${region}-docker.pkg.dev/${projectId}/blueprint-jobs/episode-gen-job:latest
                              env:
                              - name: BUCKET
                                value: ${bucket}
                              - name: SCENE_ID
                                value: ${sceneId}
                              - name: ASSETS_PREFIX
                                value: ${assetsPrefix}
                              - name: USD_PREFIX
                                value: ${usdPrefix}
                              - name: EPISODES_PREFIX
                                value: ${episodesPrefix}
                              - name: HEADLESS
                                value: "1"
                              - name: DATA_PACK_TIER
                                value: ${dataPackTier}
                              - name: EPISODES_PER_VARIATION
                                value: ${episodesPerVariation}
                              - name: REQUIRE_REAL_PHYSICS
                                value: "true"
                              - name: DATA_QUALITY_LEVEL
                                value: "production"
                              - name: ISAAC_SIM_REQUIRED
                                value: "true"
                              - name: SENSOR_CAPTURE_MODE
                                value: "isaac_sim"
                              - name: USE_MOCK_CAPTURE
                                value: "false"
                              - name: ALLOW_MOCK_DATA
                                value: "false"
                              - name: ALLOW_MOCK_CAPTURE
                                value: "false"
                              - name: BUNDLE_TIER
                                value: ${bundleTier}
                              - name: AUDIO_NARRATION_ENABLED
                                value: '${if(audioNarrationEnabled, "true", "false")}'
                              - name: SUBTITLE_GENERATION_ENABLED
                                value: '${if(subtitleGenerationEnabled, "true", "false")}'
                              - name: ENABLE_FIREBASE_UPLOAD
                                value: ${enableFirebaseUpload}
                              - name: FIREBASE_STORAGE_BUCKET
                                value: ${firebaseBucket}
                              - name: FIREBASE_SERVICE_ACCOUNT_JSON
                                value: ${firebaseServiceAccountJson}
                              - name: FIREBASE_SERVICE_ACCOUNT_PATH
                                value: ${firebaseServiceAccountPath}
                              - name: FIREBASE_UPLOAD_PREFIX
                                value: ${firebaseUploadPrefix}
                              - name: GOOGLE_APPLICATION_CREDENTIALS
                                value: /secrets/gcs/key.json
                              resources:
                                requests:
                                  cpu: "4"
                                  memory: 16Gi
                                  nvidia.com/gpu: "1"
                                limits:
                                  cpu: "8"
                                  memory: 32Gi
                                  nvidia.com/gpu: "1"
                              volumeMounts:
                              - name: gcs-credentials
                                mountPath: /secrets/gcs
                                readOnly: true
                              - name: firebase-credentials
                                mountPath: /secrets/firebase
                                readOnly: true
                              - name: dshm
                                mountPath: /dev/shm
                            volumes:
                            - name: gcs-credentials
                              secret:
                                secretName: gcs-service-account
                            - name: firebase-credentials
                              secret:
                                secretName: firebase-service-account
                            - name: dshm
                              emptyDir:
                                medium: Memory
                                sizeLimit: 16Gi
                      EOFK8S

                      echo "Created Kubernetes Job: ${JOB_NAME}"

                      # Wait for job to complete (poll every 30 seconds)
                      echo "Waiting for job completion..."
                      while true; do
                        STATUS=$(kubectl get job ${JOB_NAME} -n ${namespace} -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null || echo "")
                        FAILED=$(kubectl get job ${JOB_NAME} -n ${namespace} -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}' 2>/dev/null || echo "")

                        if [ "$STATUS" == "True" ]; then
                          echo "Job completed successfully!"
                          exit 0
                        elif [ "$FAILED" == "True" ]; then
                          echo "Job failed!"
                          kubectl logs job/${JOB_NAME} -n ${namespace} --tail=100 || true
                          exit 1
                        fi

                        echo "Job still running... ($(date))"
                        sleep 30
                      done
              timeout: '21600s'  # 6 hours
              options:
                logging: CLOUD_LOGGING_ONLY
          result: buildResult
        except:
          as: e
          steps:
            - list_episode_outputs_for_cleanup:
                call: googleapis.storage.v1.objects.list
                args:
                  bucket: ${bucket}
                  prefix: ${episodesPrefix + "/"}
                  maxResults: 1000
                result: episodeCleanupList
            - delete_episode_outputs_for_cleanup:
                for:
                  value: episodeCleanupItem
                  in: ${default(episodeCleanupList.items, [])}
                  steps:
                    - delete_episode_output:
                        call: googleapis.storage.v1.objects.delete
                        args:
                          bucket: ${bucket}
                          object: ${episodeCleanupItem.name}
            - write_episode_start_failure_marker:
                call: googleapis.storage.v1.objects.insert
                args:
                  bucket: ${bucket}
                  name: ${episodesFailedMarker}
                  uploadType: "media"
                  body: ${json.encode({
                    "scene_id": sceneId,
                    "job_name": episodeJobName,
                    "status": "failed",
                    "timestamp": time.format(sys.now()),
                    "error": {
                      "code": "episode_generation_start_failed",
                      "message": e.message,
                      "type": "workflow_failure",
                      "stack_trace": null
                    },
                    "context": {
                      "workflow_execution_id": sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),
                      "attempt_number": 1,
                      "config_context": {
                        "episodes_prefix": episodesPrefix
                      }
                    },
                    "input_params": {
                      "scene_id": sceneId,
                      "bucket": bucket
                    }
                  })}
            - raise_episode_start_error:
                raise: ${e}
        next: init_build_poll_tracking

    - init_build_poll_tracking:
        assign:
          - buildPollStartTime: '${time.format(sys.now())}'
          - buildPollAttempt: 0
          - maxPollIntervalSeconds: 300
        next: compute_build_poll_sleep

    - compute_build_poll_sleep:
        assign:
          - buildPollSleepBaseSeconds: '${if(30 * math.pow(2, buildPollAttempt) > maxPollIntervalSeconds, maxPollIntervalSeconds, 30 * math.pow(2, buildPollAttempt))}'
          - buildPollSleepJitterFactor: '${(sys.random() * 0.2) - 0.1}'
          - buildPollSleepSecondsRaw: '${int(buildPollSleepBaseSeconds * (1 + buildPollSleepJitterFactor))}'
          - buildPollSleepSeconds: '${if(buildPollSleepSecondsRaw < 1, 1, buildPollSleepSecondsRaw)}'
        next: log_build_poll_sleep

    - log_build_poll_sleep:
        call: sys.log
        args:
          text: '${"Waiting " + string(buildPollSleepSeconds) + "s before polling build status (attempt " + string(buildPollAttempt) + ") for scene " + sceneId}'
          severity: "INFO"
        next: wait_for_build

    - wait_for_build:
        call: sys.sleep
        args:
          seconds: ${buildPollSleepSeconds}
        next: check_build_status

    - check_build_status:
        call: googleapis.cloudbuild.v1.projects.builds.get
        args:
          projectId: ${projectId}
          id: ${buildResult.metadata.build.id}
        result: buildStatus
        next: build_status_switch

    - build_status_switch:
        switch:
          - condition: '${buildStatus.status == "FAILURE" or buildStatus.status == "TIMEOUT" or buildStatus.status == "CANCELLED"}'
            next: episode_job_failed
          - condition: '${buildStatus.status == "SUCCESS"}'
            next: log_episode_complete
        next: evaluate_build_poll_guard

    - evaluate_build_poll_guard:
        assign:
          - buildPollAttempt: ${buildPollAttempt + 1}
          - buildPollNow: '${time.format(sys.now())}'
          - buildPollElapsedSeconds: '${time.parse(buildPollNow) - time.parse(buildPollStartTime)}'
        next: build_poll_guard_switch

    - build_poll_guard_switch:
        switch:
          - condition: '${buildPollElapsedSeconds >= buildPollTimeoutSeconds}'
            next: build_poll_timeout_failed
        next: compute_build_poll_sleep

    - build_poll_timeout_failed:
        assign:
          - episodeEndTime: '${time.format(sys.now())}'
          - episodeDurationSeconds: '${time.parse(episodeEndTime) - time.parse(episodeStartTime)}'
          - episodeTimeoutUsageRatio: '${episodeDurationSeconds / episodeJobTimeoutSeconds}'
          - episodeTimedOut: true
        next: emit_episode_poll_timeout_metrics

    - emit_episode_poll_timeout_metrics:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "complete"
            workflow: "episode-generation-pipeline"
            job: ${episodeJobName}
            scene_id: ${sceneId}
            timeout_seconds: ${episodeJobTimeoutSeconds}
            duration_seconds: ${episodeDurationSeconds}
            timeout_usage_ratio: ${episodeTimeoutUsageRatio}
            timed_out: ${episodeTimedOut}
            status: "FAILED"
            start_time: ${episodeStartTime}
            end_time: ${episodeEndTime}
          severity: "ERROR"
        next: log_episode_poll_timeout

    - log_episode_poll_timeout:
        call: sys.log
        args:
          text: '${"ERROR: Episode generation build polling timed out after " + string(buildPollElapsedSeconds) + "s (max " + string(buildPollTimeoutSeconds) + "s, attempts=" + string(buildPollAttempt) + ") for scene " + sceneId}'
          severity: "ERROR"
        next: write_episode_poll_timeout_marker

    - write_episode_poll_timeout_marker:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${bucket}
          name: ${episodesFailedMarker}
          uploadType: "media"
          body: ${json.encode({
            "scene_id": sceneId,
            "job_name": episodeJobName,
            "status": "failed",
            "timestamp": time.format(sys.now()),
            "error": {
              "code": "episode_generation_poll_timeout",
              "message": "Polling timed out waiting for Cloud Build completion",
              "type": "workflow_timeout",
              "stack_trace": null
            },
            "context": {
              "workflow_execution_id": sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),
              "attempt_number": buildPollAttempt,
              "config_context": {
                "episodes_prefix": episodesPrefix,
                "poll_elapsed_seconds": buildPollElapsedSeconds,
                "poll_timeout_seconds": buildPollTimeoutSeconds
              }
            },
            "input_params": {
              "scene_id": sceneId,
              "bucket": bucket
            }
          })}
        next: write_legacy_failure_marker

    - log_episode_complete:
        assign:
          - episodeEndTime: '${time.format(sys.now())}'
          - episodeDurationSeconds: '${time.parse(episodeEndTime) - time.parse(episodeStartTime)}'
          - episodeTimeoutUsageRatio: '${episodeDurationSeconds / episodeJobTimeoutSeconds}'
          - episodeTimedOut: '${episodeDurationSeconds >= episodeJobTimeoutSeconds or buildStatus.status == "TIMEOUT"}'
        next: emit_episode_metrics_complete

    - emit_episode_metrics_complete:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "complete"
            workflow: "episode-generation-pipeline"
            job: ${episodeJobName}
            scene_id: ${sceneId}
            timeout_seconds: ${episodeJobTimeoutSeconds}
            duration_seconds: ${episodeDurationSeconds}
            timeout_usage_ratio: ${episodeTimeoutUsageRatio}
            timed_out: ${episodeTimedOut}
            status: "SUCCEEDED"
            start_time: ${episodeStartTime}
            end_time: ${episodeEndTime}
          severity: "INFO"
        next: log_episode_complete_message

    - log_episode_complete_message:
        call: sys.log
        args:
          text: '${"Episode generation completed for scene " + sceneId}'
          severity: "INFO"
        next: check_staging_e2e_tests

    - check_staging_e2e_tests:
        switch:
          - condition: '${runStagingE2E == true}'
            next: run_staging_e2e_job_via_cloudbuild
        next: check_quality_gates

    - run_staging_e2e_job_via_cloudbuild:
        call: googleapis.cloudbuild.v1.projects.builds.create
        args:
          projectId: ${projectId}
          body:
            steps:
              - name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
                entrypoint: 'bash'
                args:
                  - '-c'
                  - |
                    set -e

                    # Get GKE credentials
                    gcloud container clusters get-credentials ${gkeCluster} \
                      --zone=${gkeZone} \
                      --project=${projectId}

                    # Create unique job name
                    JOB_NAME="staging-e2e-${sceneId}-$(date +%s)"

                    # Create the Kubernetes Job to run staging tests in Isaac Sim
                    cat <<EOFK8S | kubectl apply -f -
                    apiVersion: batch/v1
                    kind: Job
                    metadata:
                      name: ${JOB_NAME}
                      namespace: ${namespace}
                      labels:
                        app: staging-e2e
                        scene-id: ${sceneId}
                        triggered-by: workflow
                    spec:
                      backoffLimit: 1
                      activeDeadlineSeconds: 21600  # 6 hours max
                      template:
                        metadata:
                          labels:
                            app: staging-e2e
                        spec:
                          restartPolicy: Never
                          serviceAccountName: blueprint-pipeline-sa
                          nodeSelector:
                            cloud.google.com/gke-accelerator: nvidia-tesla-t4
                          tolerations:
                          - key: nvidia.com/gpu
                            operator: Equal
                            value: present
                            effect: NoSchedule
                          containers:
                          - name: staging-e2e
                            image: ${region}-docker.pkg.dev/${projectId}/blueprint-jobs/episode-gen-job:latest
                            command: ["/bin/bash", "-lc"]
                            args:
                              - |
                                set -o pipefail

                                echo "Syncing scene data for staging tests..."
                                mkdir -p ${stagingDataRoot}/scenes/${sceneId}
                                gsutil -m rsync -r gs://${bucket}/scenes/${sceneId} ${stagingDataRoot}/scenes/${sceneId}

                                TEST_OUTPUT=$(/isaac-sim/python.sh -m pytest tests/test_pipeline_e2e_staging.py -v 2>&1)
                                TEST_STATUS=$?
                                echo "${TEST_OUTPUT}"

                                if [ "${TEST_STATUS}" -ne 0 ]; then
                                  exit ${TEST_STATUS}
                                fi

                                if echo "${TEST_OUTPUT}" | grep -E -q "SKIPPED|skipped"; then
                                  echo "ERROR: Staging E2E tests were skipped."
                                  exit 1
                                fi
                            env:
                            - name: RUN_STAGING_E2E
                              value: "1"
                            - name: PIPELINE_ENV
                              value: "production"
                            - name: SIMREADY_PHYSICS_MODE
                              value: "deterministic"
                            - name: STAGING_DATA_ROOT
                              value: ${stagingDataRoot}
                            - name: STAGING_SCENE_ID
                              value: ${sceneId}
                            - name: GOOGLE_APPLICATION_CREDENTIALS
                              value: /secrets/gcs/key.json
                            resources:
                              requests:
                                cpu: "4"
                                memory: 16Gi
                                nvidia.com/gpu: "1"
                              limits:
                                cpu: "8"
                                memory: 32Gi
                                nvidia.com/gpu: "1"
                            volumeMounts:
                            - name: gcs-credentials
                              mountPath: /secrets/gcs
                              readOnly: true
                            - name: dshm
                              mountPath: /dev/shm
                          volumes:
                          - name: gcs-credentials
                            secret:
                              secretName: gcs-service-account
                          - name: dshm
                            emptyDir:
                              medium: Memory
                              sizeLimit: 16Gi
                    EOFK8S

                    echo "Created Kubernetes Job: ${JOB_NAME}"

                    # Wait for job to complete (poll every 30 seconds)
                    echo "Waiting for staging test job completion..."
                    while true; do
                      STATUS=$(kubectl get job ${JOB_NAME} -n ${namespace} -o jsonpath='{.status.conditions[?(@.type=="Complete")].status}' 2>/dev/null || echo "")
                      FAILED=$(kubectl get job ${JOB_NAME} -n ${namespace} -o jsonpath='{.status.conditions[?(@.type=="Failed")].status}' 2>/dev/null || echo "")

                      if [ "$STATUS" == "True" ]; then
                        echo "Staging tests completed successfully!"
                        exit 0
                      elif [ "$FAILED" == "True" ]; then
                        echo "Staging tests failed!"
                        kubectl logs job/${JOB_NAME} -n ${namespace} --tail=200 || true
                        exit 1
                      fi

                      echo "Staging tests still running... ($(date))"
                      sleep 30
                    done
            timeout: '21600s'  # 6 hours
            options:
              logging: CLOUD_LOGGING_ONLY
        result: stagingBuildResult
        next: wait_for_staging_build

    - wait_for_staging_build:
        call: sys.sleep
        args:
          seconds: 30
        next: check_staging_build_status

    - check_staging_build_status:
        call: googleapis.cloudbuild.v1.projects.builds.get
        args:
          projectId: ${projectId}
          id: ${stagingBuildResult.metadata.build.id}
        result: stagingBuildStatus
        next: staging_build_status_switch

    - staging_build_status_switch:
        switch:
          - condition: '${stagingBuildStatus.status == "FAILURE" or stagingBuildStatus.status == "TIMEOUT" or stagingBuildStatus.status == "CANCELLED"}'
            next: staging_tests_failed
          - condition: '${stagingBuildStatus.status == "SUCCESS"}'
            next: staging_tests_succeeded
        next: wait_for_staging_build_poll

    - wait_for_staging_build_poll:
        call: sys.sleep
        args:
          seconds: 30
        next: check_staging_build_status

    - staging_tests_failed:
        call: sys.log
        args:
          text: '${"ERROR: Staging E2E tests failed or were skipped for scene " + sceneId}'
          severity: "ERROR"
        next: raise_staging_tests_failed

    - raise_staging_tests_failed:
        raise: '${"Staging E2E tests failed or were skipped for scene " + sceneId}'

    - staging_tests_succeeded:
        call: sys.log
        args:
          text: '${"Staging E2E tests succeeded for scene " + sceneId}'
          severity: "INFO"
        next: check_quality_gates

    - check_quality_gates:
        switch:
          - condition: '${enforceQualityGates == true}'
            next: init_quality_gate_metrics
        next: write_completion_marker

    - init_quality_gate_metrics:
        assign:
          - qualityGateStartTime: '${time.format(sys.now())}'
        next: emit_quality_gate_metrics_start

    - emit_quality_gate_metrics_start:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "start"
            workflow: "episode-generation-pipeline"
            job: ${qualityGateJobName}
            scene_id: ${sceneId}
            timeout_seconds: ${qualityGateTimeoutSeconds}
            duration_seconds: 0
            timeout_usage_ratio: 0
            timed_out: false
            status: "STARTED"
            start_time: ${qualityGateStartTime}
          severity: "INFO"
        next: run_quality_gate_job

    - run_quality_gate_job:
        try:
          call: googleapis.run.v2.projects.locations.jobs.run
          args:
            name: '${"projects/" + projectId + "/locations/" + region + "/jobs/" + qualityGateJobName}'
            body:
              overrides:
                containerOverrides:
                  - env:
                      - name: BUCKET
                        value: ${bucket}
                      - name: SCENE_ID
                        value: ${sceneId}
                      - name: EPISODES_PREFIX
                        value: ${episodesPrefix}
              timeout: '${string(qualityGateTimeoutSeconds) + "s"}'
          result: qualityGateExec
        retry:
          predicate: ${http.default_retry_predicate}
          max_retries: 5  # default per policy_configs/retry_policy.yaml
          backoff:
            initial_delay: 1
            max_delay: 60
            multiplier: 2
        except:
          as: e
          steps:
            - log_quality_gate_retry_exhausted:
                call: sys.log
                args:
                  data:
                    bp_metric: "job_retry_exhausted"
                    workflow: "episode-generation-pipeline"
                    job: ${qualityGateJobName}
                    scene_id: ${sceneId}
                    retry_max: 5
                    error: ${e.message}
                  severity: "ERROR"
            - capture_quality_gate_failure_metrics:
                assign:
                  - qualityGateEndTime: '${time.format(sys.now())}'
                  - qualityGateDurationSeconds: '${time.parse(qualityGateEndTime) - time.parse(qualityGateStartTime)}'
                  - qualityGateTimeoutUsageRatio: '${qualityGateDurationSeconds / qualityGateTimeoutSeconds}'
                  - qualityGateTimedOut: '${qualityGateDurationSeconds >= qualityGateTimeoutSeconds}'
            - emit_quality_gate_metrics_failed_on_start:
                call: sys.log
                args:
                  data:
                    bp_metric: "job_invocation"
                    event: "complete"
                    workflow: "episode-generation-pipeline"
                    job: ${qualityGateJobName}
                    scene_id: ${sceneId}
                    timeout_seconds: ${qualityGateTimeoutSeconds}
                    duration_seconds: ${qualityGateDurationSeconds}
                    timeout_usage_ratio: ${qualityGateTimeoutUsageRatio}
                    timed_out: ${qualityGateTimedOut}
                    status: "FAILED"
                    start_time: ${qualityGateStartTime}
                    end_time: ${qualityGateEndTime}
                  severity: "ERROR"
            - log_quality_gate_error:
                call: sys.log
                args:
                  text: '${"Failed to start quality gate job: " + e.message}'
                  severity: "ERROR"
            - write_quality_gate_start_failure_marker:
                call: googleapis.storage.v1.objects.insert
                args:
                  bucket: ${bucket}
                  name: ${episodesFailedMarker}
                  uploadType: "media"
                  body: ${json.encode({
                    "scene_id": sceneId,
                    "job_name": qualityGateJobName,
                    "status": "failed",
                    "timestamp": time.format(sys.now()),
                    "error": {
                      "code": "quality_gate_start_failed",
                      "message": e.message,
                      "type": "workflow_failure",
                      "stack_trace": null
                    },
                    "context": {
                      "workflow_execution_id": sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),
                      "attempt_number": 1,
                      "config_context": {
                        "episodes_prefix": episodesPrefix
                      }
                    },
                    "input_params": {
                      "scene_id": sceneId,
                      "bucket": bucket
                    }
                  })}
            - raise_quality_gate_error:
                raise: ${e}
        next: set_quality_gate_execution_name

    - set_quality_gate_execution_name:
        assign:
          - qualityGateExecutionName: '${if(qualityGateExec.metadata != null and qualityGateExec.metadata.name != null, qualityGateExec.metadata.name, qualityGateExec.name)}'
        next: wait_for_quality_gate

    - wait_for_quality_gate:
        call: googleapis.run.v2.projects.locations.jobs.executions.get
        args:
          name: ${qualityGateExecutionName}
        result: qualityGateStatus
        next: check_quality_gate_status

    - check_quality_gate_status:
        assign:
          - qualityGateState: '${if(qualityGateStatus.state != null, qualityGateStatus.state, if(qualityGateStatus.status != null, qualityGateStatus.status.state, null))}'
          - qualityGateFailedCount: '${if(qualityGateStatus.failedCount != null, qualityGateStatus.failedCount, if(qualityGateStatus.status != null, qualityGateStatus.status.failedCount, null))}'
          - qualityGateSucceededCount: '${if(qualityGateStatus.succeededCount != null, qualityGateStatus.succeededCount, if(qualityGateStatus.status != null, qualityGateStatus.status.succeededCount, null))}'
        next: quality_gate_status_switch

    - quality_gate_status_switch:
        switch:
          - condition: '${qualityGateState == "FAILED" or (qualityGateFailedCount != null and qualityGateFailedCount > 0)}'
            next: quality_gate_failed
          - condition: '${qualityGateState == "SUCCEEDED" or (qualityGateSucceededCount != null and qualityGateSucceededCount > 0)}'
            next: log_quality_gate_complete
        next: wait_for_quality_gate_poll

    - log_quality_gate_complete:
        assign:
          - qualityGateEndTime: '${time.format(sys.now())}'
          - qualityGateDurationSeconds: '${time.parse(qualityGateEndTime) - time.parse(qualityGateStartTime)}'
          - qualityGateTimeoutUsageRatio: '${qualityGateDurationSeconds / qualityGateTimeoutSeconds}'
          - qualityGateTimedOut: '${qualityGateDurationSeconds >= qualityGateTimeoutSeconds}'
        next: emit_quality_gate_metrics_complete

    - emit_quality_gate_metrics_complete:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "complete"
            workflow: "episode-generation-pipeline"
            job: ${qualityGateJobName}
            scene_id: ${sceneId}
            timeout_seconds: ${qualityGateTimeoutSeconds}
            duration_seconds: ${qualityGateDurationSeconds}
            timeout_usage_ratio: ${qualityGateTimeoutUsageRatio}
            timed_out: ${qualityGateTimedOut}
            status: "SUCCEEDED"
            start_time: ${qualityGateStartTime}
            end_time: ${qualityGateEndTime}
          severity: "INFO"
        next: write_completion_marker

    - wait_for_quality_gate_poll:
        call: sys.sleep
        args:
          seconds: 15
        next: wait_for_quality_gate

    - quality_gate_failed:
        assign:
          - qualityGateEndTime: '${time.format(sys.now())}'
          - qualityGateDurationSeconds: '${time.parse(qualityGateEndTime) - time.parse(qualityGateStartTime)}'
          - qualityGateTimeoutUsageRatio: '${qualityGateDurationSeconds / qualityGateTimeoutSeconds}'
          - qualityGateTimedOut: '${qualityGateDurationSeconds >= qualityGateTimeoutSeconds}'
        next: emit_quality_gate_metrics_failed

    - emit_quality_gate_metrics_failed:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "complete"
            workflow: "episode-generation-pipeline"
            job: ${qualityGateJobName}
            scene_id: ${sceneId}
            timeout_seconds: ${qualityGateTimeoutSeconds}
            duration_seconds: ${qualityGateDurationSeconds}
            timeout_usage_ratio: ${qualityGateTimeoutUsageRatio}
            timed_out: ${qualityGateTimedOut}
            status: "FAILED"
            start_time: ${qualityGateStartTime}
            end_time: ${qualityGateEndTime}
          severity: "ERROR"
        next: log_quality_gate_failed

    - log_quality_gate_failed:
        call: sys.log
        args:
          text: '${"ERROR: Quality gate SLIs failed for scene " + sceneId}'
          severity: "ERROR"
        next: write_quality_gate_failure_marker

    - write_quality_gate_failure_marker:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${bucket}
          name: ${episodesFailedMarker}
          uploadType: "media"
          body: ${json.encode({
            "scene_id": sceneId,
            "job_name": qualityGateJobName,
            "status": "failed",
            "timestamp": time.format(sys.now()),
            "error": {
              "code": "quality_gate_failed",
              "message": "Production SLIs failed; quality gate blocked delivery",
              "type": "workflow_failure",
              "stack_trace": null
            },
            "context": {
              "execution_id": qualityGateExecutionName,
              "workflow_execution_id": sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),
              "attempt_number": 1,
              "config_context": {
                "episodes_prefix": episodesPrefix
              }
            },
            "input_params": {
              "scene_id": sceneId,
              "bucket": bucket
            }
          })}
        result: qualityFailMarkerResult
        next: write_legacy_failure_marker

    # Write completion marker for downstream processes
    - write_completion_marker:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${bucket}
          name: ${episodesCompleteMarker}
          uploadType: "media"
          body: '${"{\"scene_id\": \"" + sceneId + "\", \"status\": \"completed\", \"timestamp\": \"" + time.format(sys.now()) + "\", \"physics_validated\": true}"}'
        result: markerResult
        next: done

    - episode_job_failed:
        # Episode generation failure is concerning but not fatal
        assign:
          - episodeEndTime: '${time.format(sys.now())}'
          - episodeDurationSeconds: '${time.parse(episodeEndTime) - time.parse(episodeStartTime)}'
          - episodeTimeoutUsageRatio: '${episodeDurationSeconds / episodeJobTimeoutSeconds}'
          - episodeTimedOut: '${episodeDurationSeconds >= episodeJobTimeoutSeconds or buildStatus.status == "TIMEOUT"}'
        next: emit_episode_metrics_failed

    - emit_episode_metrics_failed:
        call: sys.log
        args:
          data:
            bp_metric: "job_invocation"
            event: "complete"
            workflow: "episode-generation-pipeline"
            job: ${episodeJobName}
            scene_id: ${sceneId}
            timeout_seconds: ${episodeJobTimeoutSeconds}
            duration_seconds: ${episodeDurationSeconds}
            timeout_usage_ratio: ${episodeTimeoutUsageRatio}
            timed_out: ${episodeTimedOut}
            status: "FAILED"
            start_time: ${episodeStartTime}
            end_time: ${episodeEndTime}
          severity: "ERROR"
        next: log_episode_failed

    - log_episode_failed:
        call: sys.log
        args:
          text: '${"ERROR: Episode generation failed for scene " + sceneId}'
          severity: "ERROR"
        next: read_failure_marker

    - read_failure_marker:
        try:
          call: googleapis.storage.v1.objects.get
          args:
            bucket: ${bucket}
            object: ${episodesFailedMarker}
            alt: "media"
          result: episodeFailurePayload
        except:
          as: e
          steps:
            - handle_missing_episode_failure_marker:
                switch:
                  - condition: '${e.code == 404}'
                    next: write_failure_marker
                next: log_episode_failure_marker_read_error
        next: log_episode_failure_marker_payload

    - log_episode_failure_marker_read_error:
        call: sys.log
        args:
          text: '${"Failed to read .failed marker for episodes in scene " + sceneId + ": " + e.message}'
          severity: "WARNING"
        next: write_failure_marker

    - log_episode_failure_marker_payload:
        call: sys.log
        args:
          text: '${"Episode failure marker payload for scene " + sceneId + ": " + episodeFailurePayload}'
          severity: "ERROR"
        next: write_legacy_failure_marker

    - write_failure_marker:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${bucket}
          name: ${episodesFailedMarker}
          uploadType: "media"
          body: ${json.encode({
            "scene_id": sceneId,
            "job_name": episodeJobName,
            "status": "failed",
            "timestamp": time.format(sys.now()),
            "error": {
              "code": "episode_generation_failed",
              "message": "Episode generation failed",
              "type": "workflow_failure",
              "stack_trace": null
            },
            "context": {
              "execution_id": "unknown",
              "workflow_execution_id": sys.get_env("GOOGLE_CLOUD_WORKFLOW_EXECUTION_ID"),
              "attempt_number": 1,
              "config_context": {
                "episodes_prefix": episodesPrefix,
                "episodes_per_variation": episodesPerVariation
              }
            },
            "input_params": {
              "scene_id": sceneId,
              "bucket": bucket
            }
          })}
        result: failMarkerResult
        next: write_legacy_failure_marker

    - write_legacy_failure_marker:
        call: googleapis.storage.v1.objects.insert
        args:
          bucket: ${bucket}
          name: ${episodesLegacyFailedMarker}
          uploadType: "media"
          body: '${"{\"scene_id\": \"" + sceneId + "\", \"status\": \"failed\", \"timestamp\": \"" + time.format(sys.now()) + "\"}"}'
        result: legacyFailMarkerResult
        next: done_with_failure

    # =========================================================================
    # Completion
    # =========================================================================

    - done:
        return:
          status: "SUCCESS"
          scene_id: ${sceneId}
          message: '${"Episode generation completed for scene " + sceneId}'
          outputs:
            episodes_path: '${episodesPrefix}'
            completion_marker: ${episodesCompleteMarker}
            physics_validated: true

    - done_with_failure:
        return:
          status: "FAILED"
          scene_id: ${sceneId}
          message: '${"Episode generation failed for scene " + sceneId}'

    - skip:
        return:
          status: "SKIPPED"
          message: '${"Not a USD completion marker file: " + object}'
