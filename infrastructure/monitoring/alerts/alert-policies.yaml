# =============================================================================
# BlueprintPipeline - Alert Policies
# =============================================================================
# Cloud Monitoring alert policies for production monitoring.
#
# Deploy with:
#   gcloud alpha monitoring policies create --policy-from-file=alert-policies.yaml
# =============================================================================

---
# High GPU Utilization Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-gpu-high-utilization
spec:
  displayName: "[Blueprint] High GPU Utilization"
  documentation:
    content: |
      GPU utilization is above 90% for extended period.

      This may indicate:
      - Job queue backing up
      - Need for additional GPU nodes
      - Inefficient GPU usage in jobs

      Actions:
      1. Check GKE node autoscaler status
      2. Review job queue and pending jobs
      3. Consider increasing GPU node pool max size
    mimeType: text/markdown
  conditions:
    - displayName: GPU Utilization > 90%
      conditionThreshold:
        filter: >
          resource.type="k8s_node" AND
          metric.type="kubernetes.io/node/accelerator/duty_cycle"
        comparison: COMPARISON_GT
        thresholdValue: 90
        duration: 600s  # 10 minutes
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_MEAN
            crossSeriesReducer: REDUCE_MEAN
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Episode Quality Score SLI Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-episode-quality-low
spec:
  displayName: "[Blueprint] Episode Quality Score Below Threshold"
  documentation:
    content: |
      Average episode quality score fell below the production threshold.

      Actions:
      1. Inspect episode-generation logs and quality reports
      2. Verify Isaac Sim sensor capture and validation settings
      3. Regenerate episodes after addressing the root cause
    mimeType: text/markdown
  conditions:
    - displayName: Episode Quality Score < 0.85
      conditionThreshold:
        filter: >
          resource.type="global" AND
          metric.type="custom.googleapis.com/blueprint_pipeline/episode_quality_score"
        comparison: COMPARISON_LT
        thresholdValue: 0.85
        duration: 600s
        aggregations:
          - alignmentPeriod: 300s
            perSeriesAligner: ALIGN_MEAN
            crossSeriesReducer: REDUCE_MEAN
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Physics Validation Rate SLI Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-physics-validation-low
spec:
  displayName: "[Blueprint] Physics Validation Rate Below Threshold"
  documentation:
    content: |
      Physics validation coverage fell below the production threshold.

      Actions:
      1. Confirm Isaac Sim/Isaac Lab validation is enabled
      2. Investigate any heuristic-only validation fallbacks
      3. Re-run episode generation after restoring physics validation
    mimeType: text/markdown
  conditions:
    - displayName: Physics Validation Rate < 0.90
      conditionThreshold:
        filter: >
          resource.type="global" AND
          metric.type="custom.googleapis.com/blueprint_pipeline/physics_validation_score"
        comparison: COMPARISON_LT
        thresholdValue: 0.90
        duration: 600s
        aggregations:
          - alignmentPeriod: 300s
            perSeriesAligner: ALIGN_MEAN
            crossSeriesReducer: REDUCE_MEAN
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Sensor Capture Source SLI Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-sensor-capture-mock-detected
spec:
  displayName: "[Blueprint] Mock/Unknown Sensor Capture Detected"
  documentation:
    content: |
      Non-production sensor capture sources were detected (mock/unknown).

      Actions:
      1. Verify Replicator capture is enabled in Isaac Sim jobs
      2. Disable ALLOW_MOCK_CAPTURE and other dev-only flags in production
      3. Re-run episode generation after fixing capture configuration
    mimeType: text/markdown
  conditions:
    - displayName: Mock Sensor Capture Episodes > 0
      conditionThreshold:
        filter: >
          resource.type="global" AND
          metric.type="custom.googleapis.com/blueprint_pipeline/sensor_capture_source_total" AND
          metric.label.source="mock_random"
        comparison: COMPARISON_GT
        thresholdValue: 0
        duration: 600s
        aggregations:
          - alignmentPeriod: 300s
            perSeriesAligner: ALIGN_SUM
            crossSeriesReducer: REDUCE_SUM
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Workflow Job Timeout Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-workflow-job-timeout
spec:
  displayName: "[Blueprint] Workflow Job Timeout Detected"
  documentation:
    content: |
      A workflow job invocation hit or exceeded its configured timeout.

      Actions:
      1. Review workflow execution logs for duration and timeout details
      2. Confirm scene complexity or bundle tier inputs
      3. Adjust adaptive timeout overrides if needed
    mimeType: text/markdown
  conditions:
    - displayName: Job Timeout Events > 0
      conditionThreshold:
        filter: >
          resource.type="global" AND
          metric.type="logging.googleapis.com/user/blueprint_job_timeout_events"
        comparison: COMPARISON_GT
        thresholdValue: 0
        duration: 300s
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_SUM
            crossSeriesReducer: REDUCE_SUM
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Workflow Job Retry Exhausted Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-workflow-job-retry-spike
spec:
  displayName: "[Blueprint] Workflow Job Retry Spike"
  documentation:
    content: |
      Workflow job invocations are exhausting retries at a high rate.

      Actions:
      1. Review upstream API health or Cloud Run errors
      2. Check recent deploys that could cause transient failures
      3. Consider adjusting retry/backoff or rate limiting
    mimeType: text/markdown
  conditions:
    - displayName: Retry Exhausted > 3 in 10 minutes
      conditionThreshold:
        filter: >
          resource.type="global" AND
          metric.type="logging.googleapis.com/user/blueprint_job_retry_exhausted_total"
        comparison: COMPARISON_GT
        thresholdValue: 3
        duration: 600s
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_SUM
            crossSeriesReducer: REDUCE_SUM
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Workflow Job Timeout Usage Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-workflow-job-timeout-usage
spec:
  displayName: "[Blueprint] Workflow Job Timeout Usage > 80%"
  documentation:
    content: |
      Workflow job invocation duration exceeded 80% of the configured timeout.

      Actions:
      1. Review duration and timeout usage ratio logs
      2. Adjust adaptive timeout tiering if needed
      3. Investigate performance regressions in jobs
    mimeType: text/markdown
  conditions:
    - displayName: Timeout Usage Ratio > 0.8
      conditionThreshold:
        filter: >
          resource.type="global" AND
          metric.type="logging.googleapis.com/user/blueprint_job_timeout_usage_ratio"
        comparison: COMPARISON_GT
        thresholdValue: 0.8
        duration: 300s
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_MAX
            crossSeriesReducer: REDUCE_MAX
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}
  alertStrategy:
    autoClose: 1800s  # 30 minutes

---
# Job Failure Rate Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-job-failure-rate
spec:
  displayName: "[Blueprint] High Job Failure Rate"
  documentation:
    content: |
      Job failure rate is above acceptable threshold.

      This may indicate:
      - Issues with scene data
      - Infrastructure problems
      - Isaac Sim container issues
      - API quota exhaustion

      Actions:
      1. Check dead letter queue for failed jobs
      2. Review container logs for errors
      3. Verify API keys and quotas
      4. Check scene manifest validity
    mimeType: text/markdown
  conditions:
    - displayName: Job Failures > 3 in 15 minutes
      conditionThreshold:
        filter: >
          resource.type="k8s_job" AND
          resource.labels.namespace_name="blueprint" AND
          metric.type="kubernetes.io/job/failed_containers"
        comparison: COMPARISON_GT
        thresholdValue: 3
        duration: 900s  # 15 minutes
        aggregations:
          - alignmentPeriod: 300s
            perSeriesAligner: ALIGN_SUM
            crossSeriesReducer: REDUCE_SUM
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Dead Letter Queue Growing Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-dlq-growing
spec:
  displayName: "[Blueprint] Dead Letter Queue Growing"
  documentation:
    content: |
      Dead letter queue has more than 10 messages.

      Failed jobs are accumulating and need attention.

      Actions:
      1. Review DLQ messages: gsutil ls gs://${BUCKET}-dead-letter/pending/
      2. Identify common failure patterns
      3. Fix root cause issues
      4. Retry or abandon stale messages
    mimeType: text/markdown
  conditions:
    - displayName: DLQ Objects > 10
      conditionThreshold:
        filter: >
          resource.type="gcs_bucket" AND
          resource.labels.bucket_name=ends_with("-dead-letter") AND
          metric.type="storage.googleapis.com/storage/object_count"
        comparison: COMPARISON_GT
        thresholdValue: 10
        duration: 300s
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_MEAN
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Container OOM Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-container-oom
spec:
  displayName: "[Blueprint] Container Out of Memory"
  documentation:
    content: |
      Container was killed due to out of memory.

      Isaac Sim containers require significant memory (16GB+ shared memory).

      Actions:
      1. Review container resource limits
      2. Check shared memory allocation
      3. Increase memory limits if needed
      4. Consider splitting work into smaller batches
    mimeType: text/markdown
  conditions:
    - displayName: OOM Kill Detected
      conditionThreshold:
        filter: >
          resource.type="k8s_container" AND
          resource.labels.namespace_name="blueprint" AND
          metric.type="kubernetes.io/container/restart_count"
        comparison: COMPARISON_GT
        thresholdValue: 0
        duration: 60s
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_DELTA
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}
  alertStrategy:
    autoClose: 3600s  # 1 hour

---
# Workflow Execution Failure Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-workflow-failure
spec:
  displayName: "[Blueprint] Workflow Execution Failed"
  documentation:
    content: |
      A Cloud Workflow execution has failed.

      This may indicate:
      - GCS access issues
      - Cloud Run job failures
      - GKE job scheduling problems
      - Configuration errors

      Actions:
      1. Check workflow execution logs
      2. Review Cloud Run job logs
      3. Verify EventArc triggers
      4. Check service account permissions
    mimeType: text/markdown
  conditions:
    - displayName: Workflow Failed
      conditionThreshold:
        filter: >
          resource.type="workflows.googleapis.com/Workflow" AND
          metric.type="workflows.googleapis.com/finished_execution_count" AND
          metric.labels.status="FAILED"
        comparison: COMPARISON_GT
        thresholdValue: 0
        duration: 60s
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_SUM
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Storage Quota Warning
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-storage-quota
spec:
  displayName: "[Blueprint] Storage Usage High"
  documentation:
    content: |
      Storage usage is approaching quota limits.

      Actions:
      1. Review storage usage by bucket
      2. Clean up old/unused scenes
      3. Archive completed datasets to Coldline
      4. Request quota increase if needed
    mimeType: text/markdown
  conditions:
    - displayName: Storage > 100GB
      conditionThreshold:
        filter: >
          resource.type="gcs_bucket" AND
          resource.labels.bucket_name=contains("blueprint") AND
          metric.type="storage.googleapis.com/storage/total_bytes"
        comparison: COMPARISON_GT
        thresholdValue: 107374182400  # 100GB
        duration: 3600s
        aggregations:
          - alignmentPeriod: 3600s
            perSeriesAligner: ALIGN_MEAN
            crossSeriesReducer: REDUCE_SUM
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# API Quota Alert (Gemini/OpenAI)
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-api-quota
spec:
  displayName: "[Blueprint] API Rate Limit Warning"
  documentation:
    content: |
      API rate limiting may be affecting pipeline performance.

      Actions:
      1. Review API usage patterns
      2. Implement request batching
      3. Add request throttling
      4. Consider API quota increase
    mimeType: text/markdown
  conditions:
    - displayName: High Error Rate from External APIs
      conditionThreshold:
        filter: >
          resource.type="k8s_container" AND
          resource.labels.namespace_name="blueprint" AND
          metric.type="logging.googleapis.com/log_entry_count" AND
          metric.labels.severity="ERROR"
        comparison: COMPARISON_GT
        thresholdValue: 10
        duration: 300s
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_SUM
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}
