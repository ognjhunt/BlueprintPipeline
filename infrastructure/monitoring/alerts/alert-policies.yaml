# =============================================================================
# BlueprintPipeline - Alert Policies
# =============================================================================
# Cloud Monitoring alert policies for production monitoring.
#
# Deploy with:
#   gcloud alpha monitoring policies create --policy-from-file=alert-policies.yaml
# =============================================================================

---
# High GPU Utilization Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-gpu-high-utilization
spec:
  displayName: "[Blueprint] High GPU Utilization"
  documentation:
    content: |
      GPU utilization is above 90% for extended period.

      This may indicate:
      - Job queue backing up
      - Need for additional GPU nodes
      - Inefficient GPU usage in jobs

      Actions:
      1. Check GKE node autoscaler status
      2. Review job queue and pending jobs
      3. Consider increasing GPU node pool max size
    mimeType: text/markdown
  conditions:
    - displayName: GPU Utilization > 90%
      conditionThreshold:
        filter: >
          resource.type="k8s_node" AND
          metric.type="kubernetes.io/node/accelerator/duty_cycle"
        comparison: COMPARISON_GT
        thresholdValue: 90
        duration: 600s  # 10 minutes
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_MEAN
            crossSeriesReducer: REDUCE_MEAN
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Episode Quality Score SLI Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-episode-quality-low
spec:
  displayName: "[Blueprint] Episode Quality Score Below Threshold"
  documentation:
    content: |
      Average episode quality score fell below the production threshold.

      Actions:
      1. Inspect episode-generation logs and quality reports
      2. Verify Isaac Sim sensor capture and validation settings
      3. Regenerate episodes after addressing the root cause
    mimeType: text/markdown
  conditions:
    - displayName: Episode Quality Score < 0.85
      conditionThreshold:
        filter: >
          resource.type="global" AND
          metric.type="custom.googleapis.com/blueprint_pipeline/episode_quality_score"
        comparison: COMPARISON_LT
        thresholdValue: 0.85
        duration: 600s
        aggregations:
          - alignmentPeriod: 300s
            perSeriesAligner: ALIGN_MEAN
            crossSeriesReducer: REDUCE_MEAN
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Physics Validation Rate SLI Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-physics-validation-low
spec:
  displayName: "[Blueprint] Physics Validation Rate Below Threshold"
  documentation:
    content: |
      Physics validation coverage fell below the production threshold.

      Actions:
      1. Confirm Isaac Sim/Isaac Lab validation is enabled
      2. Investigate any heuristic-only validation fallbacks
      3. Re-run episode generation after restoring physics validation
    mimeType: text/markdown
  conditions:
    - displayName: Physics Validation Rate < 0.90
      conditionThreshold:
        filter: >
          resource.type="global" AND
          metric.type="custom.googleapis.com/blueprint_pipeline/physics_validation_score"
        comparison: COMPARISON_LT
        thresholdValue: 0.90
        duration: 600s
        aggregations:
          - alignmentPeriod: 300s
            perSeriesAligner: ALIGN_MEAN
            crossSeriesReducer: REDUCE_MEAN
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Sensor Capture Source SLI Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-sensor-capture-mock-detected
spec:
  displayName: "[Blueprint] Mock/Unknown Sensor Capture Detected"
  documentation:
    content: |
      Non-production sensor capture sources were detected (mock/unknown).

      Actions:
      1. Verify Replicator capture is enabled in Isaac Sim jobs
      2. Disable ALLOW_MOCK_CAPTURE and other dev-only flags in production
      3. Re-run episode generation after fixing capture configuration
    mimeType: text/markdown
  conditions:
    - displayName: Mock Sensor Capture Episodes > 0
      conditionThreshold:
        filter: >
          resource.type="global" AND
          metric.type="custom.googleapis.com/blueprint_pipeline/sensor_capture_source_total" AND
          metric.label.source="mock_random"
        comparison: COMPARISON_GT
        thresholdValue: 0
        duration: 600s
        aggregations:
          - alignmentPeriod: 300s
            perSeriesAligner: ALIGN_SUM
            crossSeriesReducer: REDUCE_SUM
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# End-to-End Success Rate SLO Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-slo-e2e-success-rate
spec:
  displayName: "[Blueprint SLO] End-to-End Success Rate Below 99%"
  documentation:
    content: |
      End-to-end workflow success rate dropped below the 99% SLO.

      This SLO represents completed Cloud Workflow executions across all pipelines.

      Actions:
      1. Identify workflows with FAILED executions in Cloud Monitoring
      2. Review workflow logs for systemic failures or dependency outages
      3. Escalate to on-call if error budget burn exceeds 2% in 24h
    mimeType: text/markdown
  conditions:
    - displayName: E2E Success Rate < 99% (1h)
      conditionMonitoringQueryLanguage:
        query: |
          fetch workflows.googleapis.com/Workflow
          | metric 'workflows.googleapis.com/finished_execution_count'
          | group_by 1h, [
              failed: sum(if(metric.label.status == "FAILED", value, 0)),
              total: sum(value)
            ]
          | value [success_rate: (total - failed) / total]
          | condition success_rate < 0.99
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Cost Anomaly Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-cost-anomaly
spec:
  displayName: "[Blueprint] Cost Anomaly Detected"
  documentation:
    content: |
      Hourly pipeline cost spiked relative to the 24h rolling average.

      Actions:
      1. Review recent pipeline runs for unusually long runtimes or retries
      2. Validate Gemini/Genie Sim usage and pricing overrides
      3. Inspect top-cost scenes in cost tracking reports
    mimeType: text/markdown
  conditions:
    - displayName: Hourly Cost > 1.5x 24h Avg
      conditionMonitoringQueryLanguage:
        query: |
          fetch global
          | metric 'custom.googleapis.com/blueprint_pipeline/pipeline_cost_total_usd'
          | align delta(1h)
          | group_by 1h, [hourly_cost: sum(value)]
          | every 1h
          | window 24h
          | group_by [], [
              avg_cost: mean(hourly_cost),
              current_cost: last(hourly_cost)
            ]
          | condition current_cost > avg_cost * 1.5
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Pipeline Failure Rate Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-pipeline-failure-rate
spec:
  displayName: "[Blueprint] Pipeline Failure Rate Above 5%"
  documentation:
    content: |
      Pipeline failure rate exceeded 5% over the last 30 minutes.

      Actions:
      1. Check recent job failures for shared root cause
      2. Inspect workload capacity and dependency health
      3. Escalate to on-call if failure rate persists
    mimeType: text/markdown
  conditions:
    - displayName: Failure Rate > 5% (30m)
      conditionMonitoringQueryLanguage:
        query: |
          fetch global
          | metric 'custom.googleapis.com/blueprint_pipeline/pipeline_runs_total'
          | group_by 30m, [
              failed: sum(if(metric.label.status == "failure", value, 0)),
              total: sum(value)
            ]
          | value [failure_rate: failed / total]
          | condition failure_rate > 0.05
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Stage Latency SLO Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-slo-stage-latency
spec:
  displayName: "[Blueprint SLO] Stage Latency P95 Above 900s"
  documentation:
    content: |
      P95 stage latency exceeded the 900 second SLO.

      Stage latency is derived from bp_metric job_invocation duration_seconds.

      Actions:
      1. Identify offending workflow/job labels in the metric explorer
      2. Check recent deploys or infrastructure saturation
      3. Consider raising per-stage timeouts or scaling resources
    mimeType: text/markdown
  conditions:
    - displayName: Job Invocation P95 Duration > 900s
      conditionThreshold:
        filter: >
          resource.type="global" AND
          metric.type="logging.googleapis.com/user/blueprint_job_invocation_duration_seconds"
        comparison: COMPARISON_GT
        thresholdValue: 900
        duration: 900s
        aggregations:
          - alignmentPeriod: 300s
            perSeriesAligner: ALIGN_PERCENTILE_95
            crossSeriesReducer: REDUCE_MAX
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Quality Pass-Rate SLO Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-slo-quality-pass-rate
spec:
  displayName: "[Blueprint SLO] Quality Gate Pass Rate Below 98%"
  documentation:
    content: |
      Quality gate pass rate dropped below the 98% SLO.

      This alert tracks quality gate failures based on validation_report.json
      thresholds and quality_gate_report.json outcomes.

      Actions:
      1. Inspect failed quality gate reports for pass_rate and average_score
      2. Verify validation thresholds in validation_report.json are met
      3. Escalate if quality gate failures persist for > 30 minutes
    mimeType: text/markdown
  conditions:
    - displayName: Quality Gate Pass Rate < 98% (1h)
      conditionMonitoringQueryLanguage:
        query: |
          fetch global
          | metric 'logging.googleapis.com/user/blueprint_job_invocation_total'
          | filter metric.label.job == "quality-gate-job"
              && metric.label.event == "complete"
          | group_by 1h, [
              failed: sum(if(metric.label.status == "FAILED", value, 0)),
              total: sum(value)
            ]
          | value [pass_rate: (total - failed) / total]
          | condition pass_rate < 0.98
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Workflow Job Timeout Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-workflow-job-timeout
spec:
  displayName: "[Blueprint] Workflow Job Timeout Detected"
  documentation:
    content: |
      A workflow job invocation hit or exceeded its configured timeout.

      Actions:
      1. Review workflow execution logs for duration and timeout details
      2. Confirm scene complexity or bundle tier inputs
      3. Adjust adaptive timeout overrides if needed
    mimeType: text/markdown
  conditions:
    - displayName: Job Timeout Events > 0
      conditionThreshold:
        filter: >
          resource.type="global" AND
          metric.type="logging.googleapis.com/user/blueprint_job_timeout_events"
        comparison: COMPARISON_GT
        thresholdValue: 0
        duration: 300s
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_SUM
            crossSeriesReducer: REDUCE_SUM
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Workflow Job Retry Exhausted Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-workflow-job-retry-spike
spec:
  displayName: "[Blueprint] Workflow Job Retry Spike"
  documentation:
    content: |
      Workflow job invocations are exhausting retries at a high rate.

      Actions:
      1. Review upstream API health or Cloud Run errors
      2. Check recent deploys that could cause transient failures
      3. Consider adjusting retry/backoff or rate limiting
    mimeType: text/markdown
  conditions:
    - displayName: Retry Exhausted > 3 in 10 minutes
      conditionThreshold:
        filter: >
          resource.type="global" AND
          metric.type="logging.googleapis.com/user/blueprint_job_retry_exhausted_total"
        comparison: COMPARISON_GT
        thresholdValue: 3
        duration: 600s
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_SUM
            crossSeriesReducer: REDUCE_SUM
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Workflow Job Failure Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-workflow-job-failure
spec:
  displayName: "[Blueprint] Workflow Job Failure Detected"
  documentation:
    content: |
      A workflow job invocation completed with FAILED status.

      Actions:
      1. Inspect workflow logs for failure root cause
      2. Check Genie Sim job metadata for metrics and failure details
      3. Verify job inputs and upstream asset generation
    mimeType: text/markdown
  conditions:
    - displayName: Job Failure Events > 0
      conditionThreshold:
        filter: >
          resource.type="global" AND
          metric.type="logging.googleapis.com/user/blueprint_job_failure_events"
        comparison: COMPARISON_GT
        thresholdValue: 0
        duration: 300s
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_SUM
            crossSeriesReducer: REDUCE_SUM
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Genie Sim SLA Violation Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-geniesim-sla-violation
spec:
  displayName: "[Blueprint] Genie Sim SLA Violation"
  documentation:
    content: |
      Genie Sim job duration exceeded the configured SLA threshold.

      Actions:
      1. Check Genie Sim job metrics in job.json
      2. Review local runtime environment for bottlenecks
      3. Adjust SLA thresholds or increase compute if needed
    mimeType: text/markdown
  conditions:
    - displayName: Genie Sim SLA Violations > 0
      conditionThreshold:
        filter: >
          resource.type="global" AND
          metric.type="logging.googleapis.com/user/blueprint_geniesim_sla_violations"
        comparison: COMPARISON_GT
        thresholdValue: 0
        duration: 300s
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_SUM
            crossSeriesReducer: REDUCE_SUM
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Workflow Job Timeout Usage Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-workflow-job-timeout-usage
spec:
  displayName: "[Blueprint] Workflow Job Timeout Usage > 80%"
  documentation:
    content: |
      Workflow job invocation duration exceeded 80% of the configured timeout.

      Actions:
      1. Review duration and timeout usage ratio logs
      2. Adjust adaptive timeout tiering if needed
      3. Investigate performance regressions in jobs
    mimeType: text/markdown
  conditions:
    - displayName: Timeout Usage Ratio > 0.8
      conditionThreshold:
        filter: >
          resource.type="global" AND
          metric.type="logging.googleapis.com/user/blueprint_job_timeout_usage_ratio"
        comparison: COMPARISON_GT
        thresholdValue: 0.8
        duration: 300s
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_MAX
            crossSeriesReducer: REDUCE_MAX
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}
  alertStrategy:
    autoClose: 1800s  # 30 minutes

---
# Job Failure Rate Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-job-failure-rate
spec:
  displayName: "[Blueprint] High Job Failure Rate"
  documentation:
    content: |
      Job failure rate is above acceptable threshold.

      This may indicate:
      - Issues with scene data
      - Infrastructure problems
      - Isaac Sim container issues
      - API quota exhaustion

      Actions:
      1. Check dead letter queue for failed jobs
      2. Review container logs for errors
      3. Verify API keys and quotas
      4. Check scene manifest validity
    mimeType: text/markdown
  conditions:
    - displayName: Job Failures > 3 in 15 minutes
      conditionThreshold:
        filter: >
          resource.type="k8s_job" AND
          resource.labels.namespace_name="blueprint" AND
          metric.type="kubernetes.io/job/failed_containers"
        comparison: COMPARISON_GT
        thresholdValue: 3
        duration: 900s  # 15 minutes
        aggregations:
          - alignmentPeriod: 300s
            perSeriesAligner: ALIGN_SUM
            crossSeriesReducer: REDUCE_SUM
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Dead Letter Queue Growing Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-dlq-growing
spec:
  displayName: "[Blueprint] Dead Letter Queue Growing"
  documentation:
    content: |
      Dead letter queue has more than 10 messages.

      Failed jobs are accumulating and need attention.

      Actions:
      1. Review DLQ messages: gsutil ls gs://${BUCKET}-dead-letter/pending/
      2. Identify common failure patterns
      3. Fix root cause issues
      4. Retry or abandon stale messages
    mimeType: text/markdown
  conditions:
    - displayName: DLQ Objects > 10
      conditionThreshold:
        filter: >
          resource.type="gcs_bucket" AND
          resource.labels.bucket_name=ends_with("-dead-letter") AND
          metric.type="storage.googleapis.com/storage/object_count"
        comparison: COMPARISON_GT
        thresholdValue: 10
        duration: 300s
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_MEAN
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Container OOM Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-container-oom
spec:
  displayName: "[Blueprint] Container Out of Memory"
  documentation:
    content: |
      Container was killed due to out of memory.

      Isaac Sim containers require significant memory (16GB+ shared memory).

      Actions:
      1. Review container resource limits
      2. Check shared memory allocation
      3. Increase memory limits if needed
      4. Consider splitting work into smaller batches
    mimeType: text/markdown
  conditions:
    - displayName: OOM Kill Detected
      conditionThreshold:
        filter: >
          resource.type="k8s_container" AND
          resource.labels.namespace_name="blueprint" AND
          metric.type="kubernetes.io/container/restart_count"
        comparison: COMPARISON_GT
        thresholdValue: 0
        duration: 60s
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_DELTA
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}
  alertStrategy:
    autoClose: 3600s  # 1 hour

---
# Workflow Execution Failure Alert
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-workflow-failure
spec:
  displayName: "[Blueprint] Workflow Execution Failed"
  documentation:
    content: |
      A Cloud Workflow execution has failed.

      This may indicate:
      - GCS access issues
      - Cloud Run job failures
      - GKE job scheduling problems
      - Configuration errors

      Actions:
      1. Check workflow execution logs
      2. Review Cloud Run job logs
      3. Verify EventArc triggers
      4. Check service account permissions
    mimeType: text/markdown
  conditions:
    - displayName: Workflow Failed
      conditionThreshold:
        filter: >
          resource.type="workflows.googleapis.com/Workflow" AND
          metric.type="workflows.googleapis.com/finished_execution_count" AND
          metric.labels.status="FAILED"
        comparison: COMPARISON_GT
        thresholdValue: 0
        duration: 60s
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_SUM
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# Storage Quota Warning
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-storage-quota
spec:
  displayName: "[Blueprint] Storage Usage High"
  documentation:
    content: |
      Storage usage is approaching quota limits.

      Actions:
      1. Review storage usage by bucket
      2. Clean up old/unused scenes
      3. Archive completed datasets to Coldline
      4. Request quota increase if needed
    mimeType: text/markdown
  conditions:
    - displayName: Storage > 100GB
      conditionThreshold:
        filter: >
          resource.type="gcs_bucket" AND
          resource.labels.bucket_name=contains("blueprint") AND
          metric.type="storage.googleapis.com/storage/total_bytes"
        comparison: COMPARISON_GT
        thresholdValue: 107374182400  # 100GB
        duration: 3600s
        aggregations:
          - alignmentPeriod: 3600s
            perSeriesAligner: ALIGN_MEAN
            crossSeriesReducer: REDUCE_SUM
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}

---
# API Quota Alert (Gemini/OpenAI)
apiVersion: monitoring.googleapis.com/v1
kind: AlertPolicy
metadata:
  name: blueprint-api-quota
spec:
  displayName: "[Blueprint] API Rate Limit Warning"
  documentation:
    content: |
      API rate limiting may be affecting pipeline performance.

      Actions:
      1. Review API usage patterns
      2. Implement request batching
      3. Add request throttling
      4. Consider API quota increase
    mimeType: text/markdown
  conditions:
    - displayName: High Error Rate from External APIs
      conditionThreshold:
        filter: >
          resource.type="k8s_container" AND
          resource.labels.namespace_name="blueprint" AND
          metric.type="logging.googleapis.com/log_entry_count" AND
          metric.labels.severity="ERROR"
        comparison: COMPARISON_GT
        thresholdValue: 10
        duration: 300s
        aggregations:
          - alignmentPeriod: 60s
            perSeriesAligner: ALIGN_SUM
  combiner: OR
  notificationChannels:
    - projects/${PROJECT_ID}/notificationChannels/${NOTIFICATION_CHANNEL}
